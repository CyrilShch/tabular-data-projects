{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 4\n",
    "### David Lowe\n",
    "### February 15, 2019\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities and Postural Transitions dataset is a classic multi-class classification situation where we are trying to predict one of the 12 possible outcomes.\n",
    "\n",
    "INTRODUCTION: The research team carried out experiments with a group of 30 volunteers who performed a protocol of activities composed of six basic activities. There are three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs). The experiment also included postural transitions that occurred between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a smartphone on the waist during the experiment execution. The research team also video-recorded the activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and 30% for the testing.\n",
    "\n",
    "In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Because the dataset has many attributes that were collinear with other attributes, we eliminated the attributes that have a collinearity measurement of 99% or higher. Iteration Take1 established the performance baseline for accuracy and processing time.\n",
    "\n",
    "In iteration Take2, we examined the feature selection technique of eliminating collinear features. We performed iterative modeling at collinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear features, we decreased the processing time and maintained a comparable level of model accuracy comparing to iteration Take1.\n",
    "\n",
    "In iteration Take3, we examined the feature selection technique of attribute importance ranking by using the Random Forest algorithm. By selecting only the most important attributes, we hoped to decrease the processing time and to maintain a similar level of accuracy compared to iteration Take1.\n",
    "\n",
    "In the current iteration Take4, we will examine the feature selection technique of Recursive Feature Elimination by using the Linear Discriminant Analysis algorithm. By limiting to only the 300 most relevant attributes, we hope to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.\n",
    "\n",
    "ANALYSIS: In iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 88.52%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top overall result and achieved an accuracy metric of 94.19%. By using the optimized parameters, the Linear Discriminant Analysis algorithm processed the testing dataset with an accuracy of 94.71%, which was even better than the training data.\n",
    "\n",
    "From the model-building perspective, the number of attributes decreased by 108, from 561 down to 453.\n",
    "\n",
    "In iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 88.04%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top overall result and achieved an accuracy metric of 92.32%. By using the optimized parameters, the Linear Discriminant Analysis algorithm processed the testing dataset with an accuracy of 93.89%, which was even better than the training data.\n",
    "\n",
    "From the model-building perspective, the number of attributes decreased by 278, from 561 down to 283. The processing time went from 7 hours 3 minutes in iteration Take1 down to 4 hours 31 minutes in Take2, which was a reduction of 35.9%.\n",
    "\n",
    "In iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.02%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top overall result and achieved an accuracy metric of 94.42%. By using the optimized parameters, the Linear Discriminant Analysis algorithm processed the testing dataset with an accuracy of 94.97%, which was even better than the training data.\n",
    "\n",
    "From the model-building perspective, the number of attributes decreased by 107, from 561 down to 454. The processing time went from 7 hours 3 minutes in iteration Take1 down to 6 hours 06 minutes in Take3, which was a reduction of 13.4%.\n",
    "\n",
    "In the current iteration Take4, the baseline performance of the machine learning algorithms achieved an average accuracy of 87.22%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top overall result and achieved an accuracy metric of 91.45%. By using the optimized parameters, the Linear Discriminant Analysis algorithm processed the testing dataset with an accuracy of 91.42%, which was even better than the training data.\n",
    "\n",
    "From the model-building perspective, the number of attributes decreased by 261, from 561 down to 300. The processing time went from 7 hours 3 minutes in iteration Take1 down to 4 hours 57 minutes in Take4, which was a reduction of 29.7%.\n",
    "\n",
    "CONCLUSION: For this iteration, the Recursive Feature Elimination technique and the Linear Discriminant Analysis algorithm achieved the best overall results while reducing the processing time. For this dataset, we should consider using the Linear Discriminant Analysis algorithm for further modeling or production use.\n",
    "\n",
    "Dataset Used: Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set\n",
    "\n",
    "Dataset ML Model: Multi-class classification with numerical attributes\n",
    "\n",
    "Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\n",
    "\n",
    "The project aims to touch on the following areas:\n",
    "\n",
    "* Document a predictive modeling problem end-to-end.\n",
    "* Explore data cleaning and transformation options\n",
    "* Explore non-ensemble and ensemble algorithms for baseline model performance\n",
    "* Explore algorithm tuning techniques for improving model performance\n",
    "\n",
    "Any predictive modeling machine learning project genrally can be broken down into about six major tasks:\n",
    "\n",
    "1. Prepare Problem\n",
    "2. Summarize Data\n",
    "3. Prepare Data\n",
    "4. Model and Evaluate Algorithms\n",
    "5. Improve Accuracy or Results\n",
    "6. Finalize Model and Present Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Prepare Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a) Load ibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import get_dummies\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.externals.joblib import load\n",
    "from sklearn.feature_selection import RFE\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create one random seed number for reproducible results\n",
    "seedNum = 888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b) Set up the email notification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_notify(msg_text):\n",
    "    sender = \"luozhi2488@gmail.com\"\n",
    "    receiver = \"dave@contactdavidlowe.com\"\n",
    "    with open('../../email_credential.txt') as f:\n",
    "        password = f.readline()\n",
    "        f.close()\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Python Multi-Class Classification Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "    server.login(sender, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Library and Data Loading has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeScript = datetime.now()\n",
    "\n",
    "widthVector = [16] * 561\n",
    "colNames = [\"attr\" + str(i) for i in range(1,562)]\n",
    "x_train_df = pd.read_csv('X_train.txt', sep=' ', names=colNames)\n",
    "y_train_df = pd.read_csv('Y_train.txt', names=[\"targetVar\"])\n",
    "xy_train_df = pd.concat([x_train_df, y_train_df], axis=1)\n",
    "x_test_df = pd.read_csv('X_test.txt', sep=' ', names=colNames)\n",
    "y_test_df = pd.read_csv('Y_test.txt', names=[\"targetVar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(xy_train_df.columns)\n",
    "\n",
    "# Set up variable totAttr for the total number of attribute columns\n",
    "totAttr = totCol-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_train_df.shape: (7767, 562)\n",
      "x_train_df.shape: (7767, 561) y_train_df.shape: (7767, 1)\n",
      "x_test_df.shape: (3162, 561) y_test_df.shape: (3162, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"xy_train_df.shape: {}\".format(xy_train_df.shape))\n",
    "print(\"x_train_df.shape: {} y_train_df.shape: {}\".format(x_train_df.shape, y_train_df.shape))\n",
    "print(\"x_test_df.shape: {} y_test_df.shape: {}\".format(x_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d) Set up the key parameters to be used in the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr\n",
    "dispCol = 3\n",
    "if totAttr % dispCol == 0 :\n",
    "    dispRow = totAttr // dispCol\n",
    "else :\n",
    "    dispRow = (totAttr // dispCol) + 1\n",
    "    \n",
    "# Set figure width to 16 and height to 12 (4:3 aspect ratio)\n",
    "fig_size = pyplot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 12\n",
    "pyplot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Library and Data Loading completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Summarize Data\n",
    "To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Data Summarization and Visualization has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a) Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.i) Peek at the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr6</th>\n",
       "      <th>attr7</th>\n",
       "      <th>attr8</th>\n",
       "      <th>attr9</th>\n",
       "      <th>attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>attr553</th>\n",
       "      <th>attr554</th>\n",
       "      <th>attr555</th>\n",
       "      <th>attr556</th>\n",
       "      <th>attr557</th>\n",
       "      <th>attr558</th>\n",
       "      <th>attr559</th>\n",
       "      <th>attr560</th>\n",
       "      <th>attr561</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043580</td>\n",
       "      <td>-0.005970</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>-0.995381</td>\n",
       "      <td>-0.988366</td>\n",
       "      <td>-0.937382</td>\n",
       "      <td>-0.995007</td>\n",
       "      <td>-0.988816</td>\n",
       "      <td>-0.953325</td>\n",
       "      <td>-0.794796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314848</td>\n",
       "      <td>-0.713308</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841559</td>\n",
       "      <td>0.179913</td>\n",
       "      <td>-0.051718</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039480</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>-0.029067</td>\n",
       "      <td>-0.998348</td>\n",
       "      <td>-0.982945</td>\n",
       "      <td>-0.971273</td>\n",
       "      <td>-0.998702</td>\n",
       "      <td>-0.983315</td>\n",
       "      <td>-0.974000</td>\n",
       "      <td>-0.802537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.603199</td>\n",
       "      <td>-0.860677</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.845092</td>\n",
       "      <td>0.180261</td>\n",
       "      <td>-0.047436</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039978</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>-0.022651</td>\n",
       "      <td>-0.995482</td>\n",
       "      <td>-0.977314</td>\n",
       "      <td>-0.984760</td>\n",
       "      <td>-0.996415</td>\n",
       "      <td>-0.975835</td>\n",
       "      <td>-0.985973</td>\n",
       "      <td>-0.798477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404427</td>\n",
       "      <td>-0.761847</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.849230</td>\n",
       "      <td>0.180610</td>\n",
       "      <td>-0.042271</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039785</td>\n",
       "      <td>-0.011809</td>\n",
       "      <td>-0.028916</td>\n",
       "      <td>-0.996194</td>\n",
       "      <td>-0.988569</td>\n",
       "      <td>-0.993256</td>\n",
       "      <td>-0.996994</td>\n",
       "      <td>-0.988526</td>\n",
       "      <td>-0.993135</td>\n",
       "      <td>-0.798477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138373</td>\n",
       "      <td>-0.491604</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848947</td>\n",
       "      <td>0.181907</td>\n",
       "      <td>-0.040826</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038758</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.998241</td>\n",
       "      <td>-0.986774</td>\n",
       "      <td>-0.993115</td>\n",
       "      <td>-0.998216</td>\n",
       "      <td>-0.986479</td>\n",
       "      <td>-0.993825</td>\n",
       "      <td>-0.801982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366214</td>\n",
       "      <td>-0.702490</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.848164</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>-0.037080</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.038988</td>\n",
       "      <td>0.004109</td>\n",
       "      <td>-0.017340</td>\n",
       "      <td>-0.997438</td>\n",
       "      <td>-0.993485</td>\n",
       "      <td>-0.996692</td>\n",
       "      <td>-0.997522</td>\n",
       "      <td>-0.993494</td>\n",
       "      <td>-0.996916</td>\n",
       "      <td>-0.801982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554902</td>\n",
       "      <td>-0.844224</td>\n",
       "      <td>0.082632</td>\n",
       "      <td>-0.143439</td>\n",
       "      <td>0.275041</td>\n",
       "      <td>-0.368224</td>\n",
       "      <td>-0.849927</td>\n",
       "      <td>0.184795</td>\n",
       "      <td>-0.035326</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.039897</td>\n",
       "      <td>-0.005324</td>\n",
       "      <td>-0.020457</td>\n",
       "      <td>-0.997024</td>\n",
       "      <td>-0.977313</td>\n",
       "      <td>-0.987782</td>\n",
       "      <td>-0.996898</td>\n",
       "      <td>-0.977450</td>\n",
       "      <td>-0.989391</td>\n",
       "      <td>-0.800606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235576</td>\n",
       "      <td>-0.571126</td>\n",
       "      <td>-0.212754</td>\n",
       "      <td>-0.230622</td>\n",
       "      <td>0.014637</td>\n",
       "      <td>-0.189512</td>\n",
       "      <td>-0.852441</td>\n",
       "      <td>0.182142</td>\n",
       "      <td>-0.036203</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.039082</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>-0.030241</td>\n",
       "      <td>-0.996662</td>\n",
       "      <td>-0.976996</td>\n",
       "      <td>-0.986672</td>\n",
       "      <td>-0.996380</td>\n",
       "      <td>-0.977594</td>\n",
       "      <td>-0.989310</td>\n",
       "      <td>-0.800606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104337</td>\n",
       "      <td>-0.432022</td>\n",
       "      <td>-0.020888</td>\n",
       "      <td>0.593996</td>\n",
       "      <td>-0.561871</td>\n",
       "      <td>0.467383</td>\n",
       "      <td>-0.851309</td>\n",
       "      <td>0.183751</td>\n",
       "      <td>-0.035176</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.039026</td>\n",
       "      <td>-0.007410</td>\n",
       "      <td>-0.027301</td>\n",
       "      <td>-0.997431</td>\n",
       "      <td>-0.973190</td>\n",
       "      <td>-0.988183</td>\n",
       "      <td>-0.997491</td>\n",
       "      <td>-0.971557</td>\n",
       "      <td>-0.990156</td>\n",
       "      <td>-0.800245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286366</td>\n",
       "      <td>-0.579474</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.080936</td>\n",
       "      <td>-0.234313</td>\n",
       "      <td>0.117797</td>\n",
       "      <td>-0.848270</td>\n",
       "      <td>0.188955</td>\n",
       "      <td>-0.030594</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.040354</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>-0.017932</td>\n",
       "      <td>-0.994906</td>\n",
       "      <td>-0.981181</td>\n",
       "      <td>-0.990046</td>\n",
       "      <td>-0.995300</td>\n",
       "      <td>-0.982483</td>\n",
       "      <td>-0.990920</td>\n",
       "      <td>-0.799717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306076</td>\n",
       "      <td>0.115919</td>\n",
       "      <td>-0.020590</td>\n",
       "      <td>-0.127730</td>\n",
       "      <td>-0.482871</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>-0.848592</td>\n",
       "      <td>0.190283</td>\n",
       "      <td>-0.027667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attr1     attr2     attr3     attr4     attr5     attr6     attr7  \\\n",
       "0  0.043580 -0.005970 -0.035054 -0.995381 -0.988366 -0.937382 -0.995007   \n",
       "1  0.039480 -0.002131 -0.029067 -0.998348 -0.982945 -0.971273 -0.998702   \n",
       "2  0.039978 -0.005153 -0.022651 -0.995482 -0.977314 -0.984760 -0.996415   \n",
       "3  0.039785 -0.011809 -0.028916 -0.996194 -0.988569 -0.993256 -0.996994   \n",
       "4  0.038758 -0.002289 -0.023863 -0.998241 -0.986774 -0.993115 -0.998216   \n",
       "5  0.038988  0.004109 -0.017340 -0.997438 -0.993485 -0.996692 -0.997522   \n",
       "6  0.039897 -0.005324 -0.020457 -0.997024 -0.977313 -0.987782 -0.996898   \n",
       "7  0.039082 -0.016047 -0.030241 -0.996662 -0.976996 -0.986672 -0.996380   \n",
       "8  0.039026 -0.007410 -0.027301 -0.997431 -0.973190 -0.988183 -0.997491   \n",
       "9  0.040354  0.004245 -0.017932 -0.994906 -0.981181 -0.990046 -0.995300   \n",
       "\n",
       "      attr8     attr9    attr10    ...       attr553   attr554   attr555  \\\n",
       "0 -0.988816 -0.953325 -0.794796    ...     -0.314848 -0.713308 -0.112754   \n",
       "1 -0.983315 -0.974000 -0.802537    ...     -0.603199 -0.860677  0.053477   \n",
       "2 -0.975835 -0.985973 -0.798477    ...     -0.404427 -0.761847 -0.118559   \n",
       "3 -0.988526 -0.993135 -0.798477    ...     -0.138373 -0.491604 -0.036788   \n",
       "4 -0.986479 -0.993825 -0.801982    ...     -0.366214 -0.702490  0.123320   \n",
       "5 -0.993494 -0.996916 -0.801982    ...     -0.554902 -0.844224  0.082632   \n",
       "6 -0.977450 -0.989391 -0.800606    ...     -0.235576 -0.571126 -0.212754   \n",
       "7 -0.977594 -0.989310 -0.800606    ...     -0.104337 -0.432022 -0.020888   \n",
       "8 -0.971557 -0.990156 -0.800245    ...     -0.286366 -0.579474  0.012954   \n",
       "9 -0.982483 -0.990920 -0.799717    ...      0.306076  0.115919 -0.020590   \n",
       "\n",
       "    attr556   attr557   attr558   attr559   attr560   attr561  targetVar  \n",
       "0  0.030400 -0.464761 -0.018446 -0.841559  0.179913 -0.051718          5  \n",
       "1 -0.007435 -0.732626  0.703511 -0.845092  0.180261 -0.047436          5  \n",
       "2  0.177899  0.100699  0.808529 -0.849230  0.180610 -0.042271          5  \n",
       "3 -0.012892  0.640011 -0.485366 -0.848947  0.181907 -0.040826          5  \n",
       "4  0.122542  0.693578 -0.615971 -0.848164  0.185124 -0.037080          5  \n",
       "5 -0.143439  0.275041 -0.368224 -0.849927  0.184795 -0.035326          5  \n",
       "6 -0.230622  0.014637 -0.189512 -0.852441  0.182142 -0.036203          5  \n",
       "7  0.593996 -0.561871  0.467383 -0.851309  0.183751 -0.035176          5  \n",
       "8  0.080936 -0.234313  0.117797 -0.848270  0.188955 -0.030594          5  \n",
       "9 -0.127730 -0.482871 -0.070670 -0.848592  0.190283 -0.027667          5  \n",
       "\n",
       "[10 rows x 562 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.ii) Dimensions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7767, 562)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.iii) Types of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attr1        float64\n",
       "attr2        float64\n",
       "attr3        float64\n",
       "attr4        float64\n",
       "attr5        float64\n",
       "attr6        float64\n",
       "attr7        float64\n",
       "attr8        float64\n",
       "attr9        float64\n",
       "attr10       float64\n",
       "attr11       float64\n",
       "attr12       float64\n",
       "attr13       float64\n",
       "attr14       float64\n",
       "attr15       float64\n",
       "attr16       float64\n",
       "attr17       float64\n",
       "attr18       float64\n",
       "attr19       float64\n",
       "attr20       float64\n",
       "attr21       float64\n",
       "attr22       float64\n",
       "attr23       float64\n",
       "attr24       float64\n",
       "attr25       float64\n",
       "attr26       float64\n",
       "attr27       float64\n",
       "attr28       float64\n",
       "attr29       float64\n",
       "attr30       float64\n",
       "              ...   \n",
       "attr533      float64\n",
       "attr534      float64\n",
       "attr535      float64\n",
       "attr536      float64\n",
       "attr537      float64\n",
       "attr538      float64\n",
       "attr539      float64\n",
       "attr540      float64\n",
       "attr541      float64\n",
       "attr542      float64\n",
       "attr543      float64\n",
       "attr544      float64\n",
       "attr545      float64\n",
       "attr546      float64\n",
       "attr547      float64\n",
       "attr548      float64\n",
       "attr549      float64\n",
       "attr550      float64\n",
       "attr551      float64\n",
       "attr552      float64\n",
       "attr553      float64\n",
       "attr554      float64\n",
       "attr555      float64\n",
       "attr556      float64\n",
       "attr557      float64\n",
       "attr558      float64\n",
       "attr559      float64\n",
       "attr560      float64\n",
       "attr561      float64\n",
       "targetVar      int64\n",
       "Length: 562, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.iv) Statistical summary of all attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr6</th>\n",
       "      <th>attr7</th>\n",
       "      <th>attr8</th>\n",
       "      <th>attr9</th>\n",
       "      <th>attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>attr553</th>\n",
       "      <th>attr554</th>\n",
       "      <th>attr555</th>\n",
       "      <th>attr556</th>\n",
       "      <th>attr557</th>\n",
       "      <th>attr558</th>\n",
       "      <th>attr559</th>\n",
       "      <th>attr560</th>\n",
       "      <th>attr561</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "      <td>7767.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038759</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>-0.018155</td>\n",
       "      <td>-0.599017</td>\n",
       "      <td>-0.634424</td>\n",
       "      <td>-0.691270</td>\n",
       "      <td>-0.623886</td>\n",
       "      <td>-0.657884</td>\n",
       "      <td>-0.740154</td>\n",
       "      <td>-0.360200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316548</td>\n",
       "      <td>-0.625132</td>\n",
       "      <td>0.016774</td>\n",
       "      <td>0.018471</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>-0.005184</td>\n",
       "      <td>-0.485936</td>\n",
       "      <td>0.050310</td>\n",
       "      <td>-0.052888</td>\n",
       "      <td>3.934595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.101996</td>\n",
       "      <td>0.099974</td>\n",
       "      <td>0.089927</td>\n",
       "      <td>0.441481</td>\n",
       "      <td>0.367558</td>\n",
       "      <td>0.321641</td>\n",
       "      <td>0.418113</td>\n",
       "      <td>0.348005</td>\n",
       "      <td>0.272619</td>\n",
       "      <td>0.499259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313899</td>\n",
       "      <td>0.302581</td>\n",
       "      <td>0.331326</td>\n",
       "      <td>0.443540</td>\n",
       "      <td>0.601208</td>\n",
       "      <td>0.477218</td>\n",
       "      <td>0.509278</td>\n",
       "      <td>0.300866</td>\n",
       "      <td>0.276196</td>\n",
       "      <td>2.160171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.987874</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.032037</td>\n",
       "      <td>-0.011209</td>\n",
       "      <td>-0.028448</td>\n",
       "      <td>-0.992140</td>\n",
       "      <td>-0.983570</td>\n",
       "      <td>-0.984661</td>\n",
       "      <td>-0.992902</td>\n",
       "      <td>-0.984131</td>\n",
       "      <td>-0.986661</td>\n",
       "      <td>-0.795613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.548129</td>\n",
       "      <td>-0.843966</td>\n",
       "      <td>-0.108225</td>\n",
       "      <td>-0.261002</td>\n",
       "      <td>-0.470267</td>\n",
       "      <td>-0.373565</td>\n",
       "      <td>-0.810953</td>\n",
       "      <td>-0.047752</td>\n",
       "      <td>-0.140560</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.038975</td>\n",
       "      <td>-0.002921</td>\n",
       "      <td>-0.019602</td>\n",
       "      <td>-0.914202</td>\n",
       "      <td>-0.827970</td>\n",
       "      <td>-0.827696</td>\n",
       "      <td>-0.924421</td>\n",
       "      <td>-0.838559</td>\n",
       "      <td>-0.852735</td>\n",
       "      <td>-0.717007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.353980</td>\n",
       "      <td>-0.710071</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>0.029079</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>-0.005503</td>\n",
       "      <td>-0.706619</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>-0.011676</td>\n",
       "      <td>-0.246026</td>\n",
       "      <td>-0.313069</td>\n",
       "      <td>-0.450478</td>\n",
       "      <td>-0.294903</td>\n",
       "      <td>-0.362671</td>\n",
       "      <td>-0.540521</td>\n",
       "      <td>0.054178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137462</td>\n",
       "      <td>-0.503837</td>\n",
       "      <td>0.167695</td>\n",
       "      <td>0.314876</td>\n",
       "      <td>0.496871</td>\n",
       "      <td>0.352690</td>\n",
       "      <td>-0.488765</td>\n",
       "      <td>0.246834</td>\n",
       "      <td>0.109507</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938491</td>\n",
       "      <td>0.911653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.991288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             attr1        attr2        attr3        attr4        attr5  \\\n",
       "count  7767.000000  7767.000000  7767.000000  7767.000000  7767.000000   \n",
       "mean      0.038759    -0.000647    -0.018155    -0.599017    -0.634424   \n",
       "std       0.101996     0.099974     0.089927     0.441481     0.367558   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%       0.032037    -0.011209    -0.028448    -0.992140    -0.983570   \n",
       "50%       0.038975    -0.002921    -0.019602    -0.914202    -0.827970   \n",
       "75%       0.044000     0.004303    -0.011676    -0.246026    -0.313069   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.945956   \n",
       "\n",
       "             attr6        attr7        attr8        attr9       attr10  \\\n",
       "count  7767.000000  7767.000000  7767.000000  7767.000000  7767.000000   \n",
       "mean     -0.691270    -0.623886    -0.657884    -0.740154    -0.360200   \n",
       "std       0.321641     0.418113     0.348005     0.272619     0.499259   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.984661    -0.992902    -0.984131    -0.986661    -0.795613   \n",
       "50%      -0.827696    -0.924421    -0.838559    -0.852735    -0.717007   \n",
       "75%      -0.450478    -0.294903    -0.362671    -0.540521     0.054178   \n",
       "max       1.000000     1.000000     0.960341     1.000000     1.000000   \n",
       "\n",
       "          ...           attr553      attr554      attr555      attr556  \\\n",
       "count     ...       7767.000000  7767.000000  7767.000000  7767.000000   \n",
       "mean      ...         -0.316548    -0.625132     0.016774     0.018471   \n",
       "std       ...          0.313899     0.302581     0.331326     0.443540   \n",
       "min       ...         -1.000000    -1.000000    -0.976580    -1.000000   \n",
       "25%       ...         -0.548129    -0.843966    -0.108225    -0.261002   \n",
       "50%       ...         -0.353980    -0.710071     0.017627     0.029079   \n",
       "75%       ...         -0.137462    -0.503837     0.167695     0.314876   \n",
       "max       ...          0.938491     0.911653     1.000000     1.000000   \n",
       "\n",
       "           attr557      attr558      attr559      attr560      attr561  \\\n",
       "count  7767.000000  7767.000000  7767.000000  7767.000000  7767.000000   \n",
       "mean      0.009239    -0.005184    -0.485936     0.050310    -0.052888   \n",
       "std       0.601208     0.477218     0.509278     0.300866     0.276196   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.987874   \n",
       "25%      -0.470267    -0.373565    -0.810953    -0.047752    -0.140560   \n",
       "50%       0.001515    -0.005503    -0.706619     0.176777     0.004583   \n",
       "75%       0.496871     0.352690    -0.488765     0.246834     0.109507   \n",
       "max       0.998702     0.991288     1.000000     0.482229     1.000000   \n",
       "\n",
       "         targetVar  \n",
       "count  7767.000000  \n",
       "mean      3.934595  \n",
       "std       2.160171  \n",
       "min       1.000000  \n",
       "25%       2.000000  \n",
       "50%       4.000000  \n",
       "75%       5.000000  \n",
       "max      12.000000  \n",
       "\n",
       "[8 rows x 562 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.v) Count missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN in the dataframe columns:\n",
      " attr1        0\n",
      "attr2        0\n",
      "attr3        0\n",
      "attr4        0\n",
      "attr5        0\n",
      "attr6        0\n",
      "attr7        0\n",
      "attr8        0\n",
      "attr9        0\n",
      "attr10       0\n",
      "attr11       0\n",
      "attr12       0\n",
      "attr13       0\n",
      "attr14       0\n",
      "attr15       0\n",
      "attr16       0\n",
      "attr17       0\n",
      "attr18       0\n",
      "attr19       0\n",
      "attr20       0\n",
      "attr21       0\n",
      "attr22       0\n",
      "attr23       0\n",
      "attr24       0\n",
      "attr25       0\n",
      "attr26       0\n",
      "attr27       0\n",
      "attr28       0\n",
      "attr29       0\n",
      "attr30       0\n",
      "            ..\n",
      "attr533      0\n",
      "attr534      0\n",
      "attr535      0\n",
      "attr536      0\n",
      "attr537      0\n",
      "attr538      0\n",
      "attr539      0\n",
      "attr540      0\n",
      "attr541      0\n",
      "attr542      0\n",
      "attr543      0\n",
      "attr544      0\n",
      "attr545      0\n",
      "attr546      0\n",
      "attr547      0\n",
      "attr548      0\n",
      "attr549      0\n",
      "attr550      0\n",
      "attr551      0\n",
      "attr552      0\n",
      "attr553      0\n",
      "attr554      0\n",
      "attr555      0\n",
      "attr556      0\n",
      "attr557      0\n",
      "attr558      0\n",
      "attr559      0\n",
      "attr560      0\n",
      "attr561      0\n",
      "targetVar    0\n",
      "Length: 562, dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print('Number of NaN in the dataframe columns:\\n', xy_train_df.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', xy_train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.vi) Summarize the levels of the class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targetVar\n",
       "1     1226\n",
       "2     1073\n",
       "3      987\n",
       "4     1293\n",
       "5     1423\n",
       "6     1413\n",
       "7       47\n",
       "8       23\n",
       "9       75\n",
       "10      60\n",
       "11      90\n",
       "12      57\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_train_df.groupby('targetVar').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b) Data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.b.i) Univariate plots to better understand each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for each attribute\n",
    "# x_train_df.hist()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot for each attribute\n",
    "# x_train_df.plot(kind='density', subplots=True, layout=(dispRow,dispCol), sharex=False, sharey=False)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker plot for each attribute\n",
    "# x_train_df.plot(kind='box', subplots=True, layout=(dispRow,dispCol), sharex=False, sharey=False)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.ii) Multivariate plots to better understand the relationships between attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot matrix\n",
    "# scatter_matrix(x_train_df)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "# fig = pyplot.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# correlations = x_train_df.corr(method='pearson')\n",
    "# cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "# fig.colorbar(cax)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Data Summarization and Visualization completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Prepare Data\n",
    "Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:\n",
    "\n",
    "* Cleaning data by removing duplicates, marking missing values and even imputing missing values.\n",
    "* Feature selection where redundant features may be removed.\n",
    "* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Data Cleaning and Transformation has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not applicable for this iteration of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of selected features: 300\n",
      "The mask of selected features:\n",
      " [ True  True  True  True  True  True  True  True  True False False False\n",
      "  True False False  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False  True  True False\n",
      "  True False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False  True\n",
      "  True  True  True  True  True False False False False False False  True\n",
      "  True  True  True False False False  True False False False False False\n",
      " False False False False False False  True  True False False False False\n",
      " False False  True  True  True  True  True  True  True False False  True\n",
      " False  True False  True  True  True  True False  True False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True False False False False False False  True  True  True  True  True\n",
      "  True False False False False False False  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False  True  True  True False False False False  True  True  True False\n",
      " False  True  True  True False  True  True  True  True  True  True  True\n",
      "  True False  True  True False  True  True  True  True  True  True  True\n",
      "  True False False  True  True False False False False False False  True\n",
      "  True  True False False  True  True  True False  True  True  True  True\n",
      "  True  True  True False False  True  True False False False False False\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True False False False  True  True  True  True False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False  True False\n",
      " False False  True False  True  True  True  True False False False False\n",
      "  True False False False  True False  True  True  True  True  True  True\n",
      " False  True  True  True False False  True False  True  True  True  True\n",
      "  True  True False  True  True False False False False False False  True\n",
      "  True  True  True False False False  True False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True False False  True  True\n",
      "  True False False False  True False  True  True  True  True  True False\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True False False  True  True  True False False  True  True  True  True\n",
      "  True False False  True  True False  True False False  True  True  True\n",
      "  True False False False  True  True False False False False False False\n",
      "  True  True False  True False  True  True False False False False False\n",
      " False  True  True  True False False  True  True False False False False\n",
      " False False False False False False  True  True  True]\n",
      "The mask of selected features:\n",
      " [  1   1   1   1   1   1   1   1   1  66   7  99   1  70  35   1   1   1\n",
      "   1   1   1   1  11 106  89  37  38  42  41  71  72  73  74   1   1   2\n",
      "   1 219 228 226   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1 117  98 208   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1 261 249 262 212 192 167   1   1   1   1   1   1   9\n",
      " 103 105 130 162 131   1   1   1   1  67  64  51   1  47  54  52  39  40\n",
      "  96  97 104 108 222  61   1   1 232 214 202 238 110  88   1   1   1   1\n",
      "   1   1   1   3 178   1   4   1  95   1   1   1   1  44   1   6 224 218\n",
      " 174  82  83 149 159  23  24  28  27   1   1   1   1 234 254 205 211 166\n",
      " 197   1   1   1   1   1   1  86   8 113 120 116  48   1   1   1   1   1\n",
      "   1   1  94 201  50 168 112 160 177 136  25  26 183   1   1   1 186 235\n",
      " 236 213   1   1   1  81 153   1   1   1 133   1   1   1   1   1   1   1\n",
      "   1 216   1   1  68   1   1   1   1   1   1   1   1 124 256   1   1  90\n",
      " 140  20  18  19  21   1   1   1  84 129   1   1   1 102   1   1   1   1\n",
      "   1   1   1  80 242   1   1   5  65  16  14  17  15   1   1   1   1   1\n",
      "   1   1   1   1 101   1   1 227 182 154   1   1   1   1 200 161 210   1\n",
      "  46  55 250 255 258  92 257 220  77  78 158 157  56  57   1   1  22  69\n",
      " 225 181  63  13   1 114  93 148   1 164   1   1   1   1  31 139 119  45\n",
      "   1 195 163 215   1 142   1   1   1   1   1   1  79   1   1   1  87 209\n",
      "   1 115   1   1   1   1   1   1  29   1   1 169 143  58 229 247 259   1\n",
      "   1   1   1 138 109 176   1 127  30 243 233 260  53 118 193 171 170 165\n",
      " 141 126 125   1   1   1   1   1   1   1  12   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1 184   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1 123   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1 172 241 194   1   1   1   1 231 132 196 137 128 185 237 245\n",
      " 239 221 203 207 151 150  32  33  75  76   1   1   1   1 134 111   1   1\n",
      "   1 204 180 179   1 191   1   1   1   1   1 135   1 121   1   1   1 217\n",
      "   1   1   1   1   1   1   1  10 122   1   1   1 146 199   1   1   1   1\n",
      "   1  34 145   1   1 156   1 240  49   1   1   1   1 155 152 244   1   1\n",
      " 107  43 251 144 190 189   1   1  36   1 198   1   1 147 173 252 206  59\n",
      "  60   1   1   1  85 223   1   1 175  91 246 100 187 188  62 230 253 248\n",
      "   1   1   1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# Using the Linear Discriminant Analysis algorithm, we perform the Recursive Feature Elimination (RFE) technique\n",
    "x_rfeVal = x_train_df.values\n",
    "y_rfeVal = y_train_df.values.ravel()\n",
    "estimator = LinearDiscriminantAnalysis()\n",
    "selector = RFE(estimator, 300)\n",
    "selector = selector.fit(x_rfeVal, y_rfeVal)\n",
    "print('The number of selected features:',selector.n_features_)\n",
    "print('The mask of selected features:\\n',selector.support_)\n",
    "print('The mask of selected features:\\n',selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7767, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr6</th>\n",
       "      <th>attr7</th>\n",
       "      <th>attr8</th>\n",
       "      <th>attr9</th>\n",
       "      <th>attr13</th>\n",
       "      <th>...</th>\n",
       "      <th>attr534</th>\n",
       "      <th>attr535</th>\n",
       "      <th>attr542</th>\n",
       "      <th>attr543</th>\n",
       "      <th>attr544</th>\n",
       "      <th>attr547</th>\n",
       "      <th>attr548</th>\n",
       "      <th>attr559</th>\n",
       "      <th>attr560</th>\n",
       "      <th>attr561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043580</td>\n",
       "      <td>-0.005970</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>-0.995381</td>\n",
       "      <td>-0.988366</td>\n",
       "      <td>-0.937382</td>\n",
       "      <td>-0.995007</td>\n",
       "      <td>-0.988816</td>\n",
       "      <td>-0.953325</td>\n",
       "      <td>0.841796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980135</td>\n",
       "      <td>-0.999240</td>\n",
       "      <td>-0.991994</td>\n",
       "      <td>-0.990877</td>\n",
       "      <td>-0.990169</td>\n",
       "      <td>-0.991994</td>\n",
       "      <td>-0.999937</td>\n",
       "      <td>-0.841559</td>\n",
       "      <td>0.179913</td>\n",
       "      <td>-0.051718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039480</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>-0.029067</td>\n",
       "      <td>-0.998348</td>\n",
       "      <td>-0.982945</td>\n",
       "      <td>-0.971273</td>\n",
       "      <td>-0.998702</td>\n",
       "      <td>-0.983315</td>\n",
       "      <td>-0.974000</td>\n",
       "      <td>0.838758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.988296</td>\n",
       "      <td>-0.999811</td>\n",
       "      <td>-0.995857</td>\n",
       "      <td>-0.996580</td>\n",
       "      <td>-0.995671</td>\n",
       "      <td>-0.995857</td>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-0.845092</td>\n",
       "      <td>0.180261</td>\n",
       "      <td>-0.047436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039978</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>-0.022651</td>\n",
       "      <td>-0.995482</td>\n",
       "      <td>-0.977314</td>\n",
       "      <td>-0.984760</td>\n",
       "      <td>-0.996415</td>\n",
       "      <td>-0.975835</td>\n",
       "      <td>-0.985973</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989255</td>\n",
       "      <td>-0.999854</td>\n",
       "      <td>-0.995034</td>\n",
       "      <td>-0.995308</td>\n",
       "      <td>-0.994868</td>\n",
       "      <td>-0.995034</td>\n",
       "      <td>-0.999973</td>\n",
       "      <td>-0.849230</td>\n",
       "      <td>0.180610</td>\n",
       "      <td>-0.042271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039785</td>\n",
       "      <td>-0.011809</td>\n",
       "      <td>-0.028916</td>\n",
       "      <td>-0.996194</td>\n",
       "      <td>-0.988569</td>\n",
       "      <td>-0.993256</td>\n",
       "      <td>-0.996994</td>\n",
       "      <td>-0.988526</td>\n",
       "      <td>-0.993135</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989413</td>\n",
       "      <td>-0.999876</td>\n",
       "      <td>-0.995224</td>\n",
       "      <td>-0.995417</td>\n",
       "      <td>-0.995951</td>\n",
       "      <td>-0.995224</td>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.848947</td>\n",
       "      <td>0.181907</td>\n",
       "      <td>-0.040826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038758</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.998241</td>\n",
       "      <td>-0.986774</td>\n",
       "      <td>-0.993115</td>\n",
       "      <td>-0.998216</td>\n",
       "      <td>-0.986479</td>\n",
       "      <td>-0.993825</td>\n",
       "      <td>0.838581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991433</td>\n",
       "      <td>-0.999902</td>\n",
       "      <td>-0.995096</td>\n",
       "      <td>-0.995645</td>\n",
       "      <td>-0.995508</td>\n",
       "      <td>-0.995096</td>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.848164</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>-0.037080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attr1     attr2     attr3     attr4     attr5     attr6     attr7  \\\n",
       "0  0.043580 -0.005970 -0.035054 -0.995381 -0.988366 -0.937382 -0.995007   \n",
       "1  0.039480 -0.002131 -0.029067 -0.998348 -0.982945 -0.971273 -0.998702   \n",
       "2  0.039978 -0.005153 -0.022651 -0.995482 -0.977314 -0.984760 -0.996415   \n",
       "3  0.039785 -0.011809 -0.028916 -0.996194 -0.988569 -0.993256 -0.996994   \n",
       "4  0.038758 -0.002289 -0.023863 -0.998241 -0.986774 -0.993115 -0.998216   \n",
       "\n",
       "      attr8     attr9    attr13    ...      attr534   attr535   attr542  \\\n",
       "0 -0.988816 -0.953325  0.841796    ...    -0.980135 -0.999240 -0.991994   \n",
       "1 -0.983315 -0.974000  0.838758    ...    -0.988296 -0.999811 -0.995857   \n",
       "2 -0.975835 -0.985973  0.834002    ...    -0.989255 -0.999854 -0.995034   \n",
       "3 -0.988526 -0.993135  0.834002    ...    -0.989413 -0.999876 -0.995224   \n",
       "4 -0.986479 -0.993825  0.838581    ...    -0.991433 -0.999902 -0.995096   \n",
       "\n",
       "    attr543   attr544   attr547   attr548   attr559   attr560   attr561  \n",
       "0 -0.990877 -0.990169 -0.991994 -0.999937 -0.841559  0.179913 -0.051718  \n",
       "1 -0.996580 -0.995671 -0.995857 -0.999981 -0.845092  0.180261 -0.047436  \n",
       "2 -0.995308 -0.994868 -0.995034 -0.999973 -0.849230  0.180610 -0.042271  \n",
       "3 -0.995417 -0.995951 -0.995224 -0.999974 -0.848947  0.181907 -0.040826  \n",
       "4 -0.995645 -0.995508 -0.995096 -0.999974 -0.848164  0.185124 -0.037080  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the unselected attributes from the training dataframes\n",
    "x_train_df = x_train_df[x_train_df.columns[selector.support_]]\n",
    "print(x_train_df.shape)\n",
    "x_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3162, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr6</th>\n",
       "      <th>attr7</th>\n",
       "      <th>attr8</th>\n",
       "      <th>attr9</th>\n",
       "      <th>attr13</th>\n",
       "      <th>...</th>\n",
       "      <th>attr534</th>\n",
       "      <th>attr535</th>\n",
       "      <th>attr542</th>\n",
       "      <th>attr543</th>\n",
       "      <th>attr544</th>\n",
       "      <th>attr547</th>\n",
       "      <th>attr548</th>\n",
       "      <th>attr559</th>\n",
       "      <th>attr560</th>\n",
       "      <th>attr561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030914</td>\n",
       "      <td>-0.008927</td>\n",
       "      <td>0.040382</td>\n",
       "      <td>-0.938504</td>\n",
       "      <td>-0.944626</td>\n",
       "      <td>-0.759334</td>\n",
       "      <td>-0.952398</td>\n",
       "      <td>-0.950281</td>\n",
       "      <td>-0.802483</td>\n",
       "      <td>0.728511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.770610</td>\n",
       "      <td>-0.970958</td>\n",
       "      <td>-0.890169</td>\n",
       "      <td>-0.907480</td>\n",
       "      <td>-0.895518</td>\n",
       "      <td>-0.890169</td>\n",
       "      <td>-0.994105</td>\n",
       "      <td>-0.720559</td>\n",
       "      <td>0.276779</td>\n",
       "      <td>-0.051074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042548</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>-0.026236</td>\n",
       "      <td>-0.975516</td>\n",
       "      <td>-0.977502</td>\n",
       "      <td>-0.960146</td>\n",
       "      <td>-0.986694</td>\n",
       "      <td>-0.978983</td>\n",
       "      <td>-0.966820</td>\n",
       "      <td>0.770927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.924461</td>\n",
       "      <td>-0.995727</td>\n",
       "      <td>-0.951981</td>\n",
       "      <td>-0.938387</td>\n",
       "      <td>-0.938230</td>\n",
       "      <td>-0.951981</td>\n",
       "      <td>-0.998272</td>\n",
       "      <td>-0.698684</td>\n",
       "      <td>0.281322</td>\n",
       "      <td>-0.076825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038297</td>\n",
       "      <td>-0.011660</td>\n",
       "      <td>-0.025643</td>\n",
       "      <td>-0.993922</td>\n",
       "      <td>-0.979215</td>\n",
       "      <td>-0.973030</td>\n",
       "      <td>-0.994298</td>\n",
       "      <td>-0.980535</td>\n",
       "      <td>-0.977508</td>\n",
       "      <td>0.837921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.975209</td>\n",
       "      <td>-0.999504</td>\n",
       "      <td>-0.985692</td>\n",
       "      <td>-0.983452</td>\n",
       "      <td>-0.983649</td>\n",
       "      <td>-0.985692</td>\n",
       "      <td>-0.999831</td>\n",
       "      <td>-0.703355</td>\n",
       "      <td>0.280062</td>\n",
       "      <td>-0.072302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036205</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.025240</td>\n",
       "      <td>-0.994845</td>\n",
       "      <td>-0.981534</td>\n",
       "      <td>-0.976175</td>\n",
       "      <td>-0.995169</td>\n",
       "      <td>-0.983020</td>\n",
       "      <td>-0.980785</td>\n",
       "      <td>0.837921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976297</td>\n",
       "      <td>-0.999458</td>\n",
       "      <td>-0.985565</td>\n",
       "      <td>-0.986022</td>\n",
       "      <td>-0.984445</td>\n",
       "      <td>-0.985565</td>\n",
       "      <td>-0.999850</td>\n",
       "      <td>-0.699545</td>\n",
       "      <td>0.284093</td>\n",
       "      <td>-0.070079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038034</td>\n",
       "      <td>-0.013437</td>\n",
       "      <td>-0.032899</td>\n",
       "      <td>-0.993955</td>\n",
       "      <td>-0.977493</td>\n",
       "      <td>-0.984290</td>\n",
       "      <td>-0.994006</td>\n",
       "      <td>-0.977354</td>\n",
       "      <td>-0.985899</td>\n",
       "      <td>0.838651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.977007</td>\n",
       "      <td>-0.999429</td>\n",
       "      <td>-0.990502</td>\n",
       "      <td>-0.990752</td>\n",
       "      <td>-0.988634</td>\n",
       "      <td>-0.990502</td>\n",
       "      <td>-0.999925</td>\n",
       "      <td>-0.692849</td>\n",
       "      <td>0.290701</td>\n",
       "      <td>-0.066849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attr1     attr2     attr3     attr4     attr5     attr6     attr7  \\\n",
       "0  0.030914 -0.008927  0.040382 -0.938504 -0.944626 -0.759334 -0.952398   \n",
       "1  0.042548  0.001079 -0.026236 -0.975516 -0.977502 -0.960146 -0.986694   \n",
       "2  0.038297 -0.011660 -0.025643 -0.993922 -0.979215 -0.973030 -0.994298   \n",
       "3  0.036205 -0.018148 -0.025240 -0.994845 -0.981534 -0.976175 -0.995169   \n",
       "4  0.038034 -0.013437 -0.032899 -0.993955 -0.977493 -0.984290 -0.994006   \n",
       "\n",
       "      attr8     attr9    attr13    ...      attr534   attr535   attr542  \\\n",
       "0 -0.950281 -0.802483  0.728511    ...    -0.770610 -0.970958 -0.890169   \n",
       "1 -0.978983 -0.966820  0.770927    ...    -0.924461 -0.995727 -0.951981   \n",
       "2 -0.980535 -0.977508  0.837921    ...    -0.975209 -0.999504 -0.985692   \n",
       "3 -0.983020 -0.980785  0.837921    ...    -0.976297 -0.999458 -0.985565   \n",
       "4 -0.977354 -0.985899  0.838651    ...    -0.977007 -0.999429 -0.990502   \n",
       "\n",
       "    attr543   attr544   attr547   attr548   attr559   attr560   attr561  \n",
       "0 -0.907480 -0.895518 -0.890169 -0.994105 -0.720559  0.276779 -0.051074  \n",
       "1 -0.938387 -0.938230 -0.951981 -0.998272 -0.698684  0.281322 -0.076825  \n",
       "2 -0.983452 -0.983649 -0.985692 -0.999831 -0.703355  0.280062 -0.072302  \n",
       "3 -0.986022 -0.984445 -0.985565 -0.999850 -0.699545  0.284093 -0.070079  \n",
       "4 -0.990752 -0.988634 -0.990502 -0.999925 -0.692849  0.290701 -0.066849  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the unselected attributes from the validation dataframes\n",
    "x_test_df = x_test_df[x_test_df.columns[selector.support_]]\n",
    "print(x_test_df.shape)\n",
    "x_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c) Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not applicable for this iteration of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d) Display the Final Datasets for Model-Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (7767, 300) y_train.shape: (7767,)\n",
      "x_test.shape: (3162, 300) y_test.shape: (3162,)\n"
     ]
    }
   ],
   "source": [
    "# We finalize the training and testing datasets for the modeling activities\n",
    "x_train = x_train_df.values\n",
    "y_train = y_train_df.values.ravel()\n",
    "x_test = x_test_df.values\n",
    "y_test = y_test_df.values.ravel()\n",
    "print(\"x_train.shape: {} y_train.shape: {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test.shape: {} y_test.shape: {}\".format(x_test.shape, y_test.shape))\n",
    "email_notify(\"Data Cleaning and Transformation completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model and Evaluate Algorithms\n",
    "After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the training. The typical evaluation tasks include:\n",
    "\n",
    "* Defining test options such as cross validation and the evaluation metric to use.\n",
    "* Spot checking a suite of linear and nonlinear machine learning algorithms.\n",
    "* Comparing the estimated accuracy of algorithms.\n",
    "\n",
    "For this project, we will evaluate one linear, two non-linear and four ensemble algorithms:\n",
    "\n",
    "Linear Algorithm: Linear Discriminant Analysis\n",
    "\n",
    "Non-Linear Algorithms: Decision Trees (CART) and k-Nearest Neighbors\n",
    "\n",
    "Ensemble Algorithms: Bagged Decision Trees, Random Forest, Extra Trees, and Stochastic Gradient Boosting\n",
    "\n",
    "The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a) Set test options and evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Model building and evaluation has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algorithms using 10-fold cross validation\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Algorithms Spot-Checking Array\n",
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('CART', DecisionTreeClassifier(random_state=seedNum)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('BDT', BaggingClassifier(random_state=seedNum)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seedNum)))\n",
    "models.append(('ET', ExtraTreesClassifier(random_state=seedNum)))\n",
    "models.append(('GBM', GradientBoostingClassifier(random_state=seedNum)))\n",
    "results = []\n",
    "names = []\n",
    "metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.914520 (0.044949)\n",
      "Model training time: 0:00:04.404094\n",
      "CART: 0.817319 (0.047386)\n",
      "Model training time: 0:00:24.161685\n",
      "KNN: 0.849373 (0.040156)\n",
      "Model training time: 0:00:14.548450\n",
      "BDT: 0.867018 (0.050125)\n",
      "Model training time: 0:02:18.491770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 0.875901 (0.041048)\n",
      "Model training time: 0:00:08.384474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET: 0.884777 (0.029895)\n",
      "Model training time: 0:00:01.950677\n",
      "GBM: 0.897004 (0.041965)\n",
      "Model training time: 0:31:58.667867\n",
      "Average metrics (accuracy) from all models: 0.8722730357119163\n"
     ]
    }
   ],
   "source": [
    "# Generate model in turn\n",
    "for name, model in models:\n",
    "\temail_notify(\"Algorithm \"+name+\" modeling has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "\tstartTimeModule = datetime.now()\n",
    "\tkfold = KFold(n_splits=num_folds, random_state=seedNum)\n",
    "\tcv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmetrics.append(cv_results.mean())\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "\tprint ('Model training time:',(datetime.now() - startTimeModule))\n",
    "\temail_notify(\"Algorithm \"+name+\" modeling completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "print ('Average metrics ('+scoring+') from all models:',np.mean(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b) Spot-checking baseline algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison - Spot Checking')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Model building and evaluation completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Improve Accuracy\n",
    "After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a) Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the comparison array\n",
    "results = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.914510 using {}\n",
      "0.914510 (0.044955) with: {}\n",
      "Model training time: 0:00:04.891610\n"
     ]
    }
   ],
   "source": [
    "# Tuning algorithm #1 - Linear Discriminant Analysis\n",
    "email_notify(\"Algorithm #1 tuning has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "startTimeModule = datetime.now()\n",
    "paramGrid1 = dict()\n",
    "model1 = LinearDiscriminantAnalysis()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seedNum)\n",
    "grid1 = GridSearchCV(estimator=model1, param_grid=paramGrid1, scoring=scoring, cv=kfold)\n",
    "grid_result1 = grid1.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result1.best_score_, grid_result1.best_params_))\n",
    "results.append(grid_result1.cv_results_['mean_test_score'])\n",
    "names.append('LDA')\n",
    "means = grid_result1.cv_results_['mean_test_score']\n",
    "stds = grid_result1.cv_results_['std_test_score']\n",
    "params = grid_result1.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print ('Model training time:',(datetime.now() - startTimeModule))\n",
    "email_notify(\"Algorithm #1 tuning completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.897515 using {'n_estimators': 600}\n",
      "0.896614 (0.044183) with: {'n_estimators': 200}\n",
      "0.897386 (0.040251) with: {'n_estimators': 400}\n",
      "0.897515 (0.041357) with: {'n_estimators': 600}\n",
      "0.897515 (0.041357) with: {'n_estimators': 800}\n",
      "Model training time: 4:17:50.837679\n"
     ]
    }
   ],
   "source": [
    "# Tuning algorithm #2 - Stochastic Gradient Boosting\n",
    "email_notify(\"Algorithm #2 tuning has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "startTimeModule = datetime.now()\n",
    "paramGrid2 = dict(n_estimators=np.array([200,400,600,800]))\n",
    "model2 = GradientBoostingClassifier(random_state=seedNum)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seedNum)\n",
    "grid2 = GridSearchCV(estimator=model2, param_grid=paramGrid2, scoring=scoring, cv=kfold)\n",
    "grid_result2 = grid2.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\n",
    "results.append(grid_result2.cv_results_['mean_test_score'])\n",
    "names.append('GBM')\n",
    "means = grid_result2.cv_results_['mean_test_score']\n",
    "stds = grid_result2.cv_results_['std_test_score']\n",
    "params = grid_result2.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print ('Model training time:',(datetime.now() - startTimeModule))\n",
    "email_notify(\"Algorithm #2 tuning completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b) Compare Algorithms After Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7gAAAMCCAYAAABDaGpIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xv07Xdd3/nX24RgO0YIyeGWpIQOmdHT4Tb+jG1HG6aMITAdIuBYMgjB4VK7ZOgMw1QojqFBGh3pYLGoCzVAFAhIvUQFQ4xQwKGYXySJpDRwiOPkENADgRCKgKHv+WN/D25+nsvOjRPe5/FYa6/zvXy+t31+a531PN/v3r/q7gAAAMDXu2840icAAAAAdwWBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAGOIlX1uqr6sbtp30+rqnccYv1jqmrv3XHsr3dV9c+q6heO9HlMVlXvqKqnHenzAODuJXABBqqqd1XVp6vq3l+rY3b3G7r7rLVz6Kp62Nfq+LXy/Kr6YFX9x6raW1W/UlUP/1qdwx3V3f+iu599pM9j3fIz9IWq+lxVfbKqfrWqHnQn93nQn4kl8j+3vL5QVV9em7/uzhw3Sbr7rO5+w53dDwD3bAIXYJiqOi3JdyXpJE/8Gh3z2K/FcQ7jXyX5J0men+R+Sf6LJL+e5L8/kid1OPeQ9+5gntfd35TVe3nfJK+8uw60RP43Lcf7wSTv2z/f3X/r7jouALMIXIB5npHk3yV5XZLzDjWwqv5pVX28qm6qqmev32GrqvtU1cVVta+q/qSqfqSqvmFZ98yq+v2qemVV3Zzkpcuy9y7r370c4prlDtw/XDvm/15Vf7Yc9wfWlr+uqn6mqt6+bPP7VfXAqvqp5W70f6iqRx/kOk5P8kNJzu3u3+vuL3b355e7yj9+O6/nM1V1Q1X93WX5jcv5nrfjXH+uqi6vqlur6t9W1UPW1v+rZbvPVtVVVfVda+teWlVvrapfrqrPJnnmsuyXl/XfuKz71HIuV1bVA5Z1D66qS6vq5qraU1XP2bHftyzXeGtVXVdVW4f6+99Ud9+c5N8k+a82eC8ftrwftyx3ft+8LD/oz8Qmlv32jmXvrapnLtPPXo67/nd41h0c+58v42+t1aPNP1tVr7t97xoAR4LABZjnGUnesLwetz+Odqqqs5O8IMl/l+RhSc7cMeSnk9wnyd9c1j0jyQ+srf+OJDckuX+Sl69v2N1/b5l85HIH7s3L/AOXfZ6c5FlJXl1VJ6xt+n1JfiTJSUm+mOR9Sf5wmX9rkv/7INf82CR7u/sPDrJ+0+u5NsmJSd6Y5JIk357Ve/P9Sf51VX3T2vinJXnZcm5XZ/V+73dlkkdldSf5jUl+paq+cW39Ocv13HfHdsnqPyXuk+TU5Vx+MMmfL+velGRvkgcn+d4k/6KqHru27ROX875vkkuT/OtDvB8bq6qTkjwlyQeWRYd6L1+W5B1JTkhyyjL2UD8Td6W/m+SPsnrfXpnkF+/g2Dcl+f1l3Y9l9fcPwNcBgQswSFV9Z5KHJHlLd1+V5KNJ/qeDDP++JK/t7uu6+/NJ/vnafo5J8g+TvLi7b+3u/zfJv0zy9LXtb+run+7u27r7z7OZv0hyQXf/RXe/LcnnkvyXa+t/rbuv6u4vJPm1JF/o7ou7+8tJ3pzkgHdwswqRjx/soBtezx9392vXjnXqcq5f7O53JPlSVrG7329397u7+4tJXpLk71TVqUnS3b/c3Z9a3pt/meTeO67zfd396939nw7w3v3Fcj0P6+4vL+/HZ5d9f2eSH+7uL3T31Ul+Ycc1vLe737Zcwy8leeTB3pMNvaqqPpPkmqze3xds8F7+RVY/gw9ezvO9d/Icbo+PdvdFy/W/PskpS5xvPLaq/mZW79tLu/tL3f3uJL/9tTl9AO4sgQswy3lJ3tHdn1zm35iDP6b84CQ3rs2vT5+U5Lgkf7K27E+yuvN6oPGb+lR337Y2//kk63dF/3Rt+s8PML8+9qv2m+RQX4C0yfXsPFa6+1DH/8r1d/fnktyc1Xu6/zHsDy2P6X4mq7udJx1o2wP4pSSXJbmkVo+O/19Vda9l3zd3962HuIZPrE1/Psk31gE+41tf/YVOP3eIc3l+d9+3u0/u7qd1974c/r38p0kqyR8sj0n/z4fY/11t5/UnB/+ZOdjYB2f1c7r+Hw935GcdgCNA4AIMUVV/Lau7smdW1Seq6hNJ/rckj6yqA93J+3hWj5Dud+ra9Cfzl3fi9vsbST62Nv9Vn4c8wq7I6g7cwT5zusn13F5feb+WR5fvl+Sm5fO2P5zV38UJ3X3fJLdkFX37HfS9W+5u//Pu3p3VY7T/IKtHgG9Kcr+qOv7OXsP6Fzp19w/ezs0P+V529ye6+znd/eAk/yjJz9Rd823a/zFJquqvry174F2w350+nuTEHY+Un3qwwQDcswhcgDm+J8mXk+zO6vOfj0ryrUnek1Ug7fSWJD9QVd+6RMOP7l+xPLb5liQvr6rjly9QekGSX74d5/OnWX1G827X3R9J8jNJ3lSr37d73PJlTU+tqhfdRdez0xOq6jur6risPnf6/u6+McnxSW5Lsi/JsVX1o0m+edOdVtV/W1UPXx4F/mxWMfnlZd//T5ILl2t7RFafY/6a/uqbw72XVfU/VtX+/zj5dFYx/+Vl/s78THxieX1/VR1TVc/NV0f2XaK7P5rVZ3PPX36OvjP38G/iBuAvCVyAOc7L6jO1/99yF+0T3f2JrL5o6Gk7H1Xt7rcneVWSdybZk9UXOiWrL3dKkv8lq7tmNyR5b1aPO190O87npUlev3xL7ffdwWu6PZ6f1bW+Oslnsvr88ZOS/Oay/s5ez05vTHJ+Vo8mf1tWXzqVrB4vfnuSD2f16O4XcvsecX1gVl9A9dkkH0ryb/OXIX5uktOyupv7a0nO7+7L78Q13FGHei+/Pcn7q+pzWX3R1T/p7j9e1r00d/Bnors7yXOS/LOs7iI/LMn77+R1HMy5Sf5eVo++n5/VZ7K/eMgtALhHqNW/FwAc7arqW5N8MMm9d3xOlh2WXxmzt7t/5EifC3e/qvo3Sa7u7pcd6XMB4NDcwQU4ilXVk5bHME9I8hNJflPccrSrqjOq6qFV9Q1V9YSsPgf9G0f6vAA4PIELcHT7R1l9VvSjWX1O8h8f2dOBe4QHJ3l3kluz+h25z+nua4/sKQGwCY8oAwAAMII7uAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEY49kifwF3hpJNO6tNOO+1InwYAAAB3g6uuuuqT3b3rcONGBO5pp52W7e3tI30aAAAA3A2q6k82GecRZQAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjbBS4VXV2VV1fVXuq6kUHWP+Qqrqiqq6tqndV1Slr636nqj5TVb+1Y5vXVdUfV9XVy+tRy/Kqqlctx7q2qv7rO3uRAAAAzHfYwK2qY5K8Osnjk+xOcm5V7d4x7BVJLu7uRyS5IMmFa+t+MsnTD7L7/6O7H7W8rl6WPT7J6cvruUl+dtOLAQAA4Oi1yR3cM5Ls6e4buvtLSS5Jcs6OMbuTXLFMv3N9fXdfkeTW23FO52QVy93d/y7JfavqQbdjewAAAI5CmwTuyUluXJvfuyxbd02SpyzTT0pyfFWduMG+X748hvzKqrr37TgeAAAAfJVNArcOsKx3zL8wyZlV9YEkZyb5WJLbDrPfFyf5liTfnuR+SX74dhwvVfXcqtququ19+/Yd5lAAAABMt0ng7k1y6tr8KUluWh/Q3Td195O7+9FJXrIsu+VQO+3ujy+PIX8xyWuzehR6o+Mt27+mu7e6e2vXrl0bXAYAAACTbRK4VyY5vaoeWlXHJXlqkkvXB1TVSVW1f18vTnLR4Xa6/3O1VVVJvifJB5dVlyZ5xvJtyn87yS3d/fGNrgYAAICj1rGHG9Ddt1XV85JcluSYJBd193VVdUGS7e6+NMljklxYVZ3k3Ul+aP/2VfWerB5F/qaq2pvkWd19WZI3VNWurB5JvjrJDy6bvC3JE5LsSfL5JD9wl1wpAAAAo1X3X/l469edra2t3t7ePtKnAQAAwN2gqq7q7q3DjdvkEWUAAAC4xxO4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABjh2CN9AsAd9NL7HOkzuGd56S1H+gwAADjCBC58vRJ0AADwVTyiDAAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMsFHgVtXZVXV9Ve2pqhcdYP1DquqKqrq2qt5VVaesrfudqvpMVf3Wjm3esOzzg1V1UVXda1n+mKq6paquXl4/emcvEgAAgPkOG7hVdUySVyd5fJLdSc6tqt07hr0iycXd/YgkFyS5cG3dTyZ5+gF2/YYk35Lk4Un+WpJnr617T3c/anldsOnFAAAAcPTa5A7uGUn2dPcN3f2lJJckOWfHmN1Jrlim37m+vruvSHLrzp1299t6keQPkpyycwwAAABsapPAPTnJjWvze5dl665J8pRl+klJjq+qEzc5geXR5Kcn+Z21xX+nqq6pqrdX1d/aZD8AAAAc3TYJ3DrAst4x/8IkZ1bVB5KcmeRjSW7b8Bx+Jsm7u/s9y/wfJnlIdz8yyU8n+fUDnlTVc6tqu6q29+3bt+GhAAAAmGqTwN2b5NS1+VOS3LQ+oLtv6u4nd/ejk7xkWXbL4XZcVecn2ZXkBWv7+mx3f26ZfluSe1XVSTu37e7XdPdWd2/t2rVrg8sAAABgsk0C98okp1fVQ6vquCRPTXLp+oCqOqmq9u/rxUkuOtxOq+rZSR6X5Nzu/k9ryx9YVbVMn7Gc46c2uRgAAACOXocN3O6+LcnzklyW5ENJ3tLd11XVBVX1xGXYY5JcX1UfTvKAJC/fv31VvSfJryR5bFXtrarHLat+bhn7vh2/Duh7k3ywqq5J8qokT12+iAoAAAAOqia049bWVm9vbx/p0wAAAOBuUFVXdffW4cZt8ogyAAAA3OMJXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEbYKHCr6uyqur6q9lTViw6w/iFVdUVVXVtV76qqU9bW/U5VfaaqfmvHNg+tqvdX1Ueq6s1Vddyy/N7L/J5l/Wl37hIBAAA4Ghw2cKvqmCSvTvL4JLuTnFtVu3cMe0WSi7v7EUkuSHLh2rqfTPL0A+z6J5K8srtPT/LpJM9alj8ryae7+2FJXrmMAwAAgEPa5A7uGUn2dPcN3f2lJJckOWfHmN1Jrlim37m+vruvSHLr+uCqqiR/P8lbl0WvT/I9y/Q5y3yW9Y9dxgMAAMBBbRK4Jye5cW1+77Js3TVJnrJMPynJ8VV14iH2eWKSz3T3bQfY51eOt6y/ZRkPAAAAB7VJ4B7o7mnvmH9hkjOr6gNJzkzysSS3/ZWtNtvnJsdLVT23qraranvfvn2HOBQAAABHg00Cd2+SU9fmT0ly0/qA7r6pu5/c3Y9O8pJl2S2H2Ocnk9y3qo49wD6/crxl/X2S3LxzB939mu7e6u6tXbt2bXAZAAAATLZJ4F6Z5PTlW4+PS/LUJJeuD6iqk6pq/75enOSiQ+2wuzurz+p+77LovCS/sUxfusxnWf97y3gAAAA4qMMG7vI52OcluSzJh5K8pbuvq6oLquqJy7DHJLm+qj6c5AFJXr5/+6p6T5JfyerLovZW1eOWVT+c5AVVtSerz9j+4rL8F5OcuCx/QZK/8muJAAAAYKeacHN0a2urt7e3j/RpAAAAcDeoqqu6e+tw4zZ5RBkAAADu8QQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAI2wUuFV1dlVdX1V7qupFB1j/kKq6oqqurap3VdUpa+vOq6qPLK/zlmXHV9XVa69PVtVPLeueWVX71tY9+666WAAAAOY69nADquqYJK9O8t1J9ia5sqou7e5/vzbsFUku7u7XV9XfT3JhkqdX1f2SnJ9kK0knuWrZ9tNJHrV2jKuS/Ora/t7c3c+7k9cGAADAUWSTO7hnJNnT3Td095eSXJLknB1jdie5Ypl+59r6xyW5vLtvXqL28iRnr29YVacnuX+S99yxSwAAAIDNAvfkJDeuze9dlq27JslTluknJTm+qk7ccNtzs7pj22vLnrI87vzWqjr1QCdVVc+tqu2q2t63b98GlwEAAMBkmwRuHWBZ75h/YZIzq+oDSc5M8rEkt2247VOTvGlt/jeTnNbdj0jyu0lef6CT6u7XdPdWd2/t2rXr8FcBAADAaJsE7t4k63dRT0ly0/qA7r6pu5/c3Y9O8pJl2S2H27aqHpnk2O6+am1fn+ruLy6zP5/k2za/HAAAAI5WmwTulUlOr6qHVtVxWd1xvXR9QFWdVFX79/XiJBct05clOauqTqiqE5KctSzb79x89d3bVNWD1mafmORDm14MAAAAR6/Dfotyd99WVc/LKkyPSXJRd19XVRck2e7uS5M8JsmFVdVJ3p3kh5Ztb66ql2UVyUlyQXffvLb770vyhB2HfH5VPTGrR5xvTvLMO3pxAAAAHD3qq7/b6evT1tZWb29vH+nTAAAA4G5QVVd199bhxm3yiDIAAADc4wlcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAI2wUuFV1dlVdX1V7qupFB1j/kKq6oqqurap3VdUpa+vOq6qPLK/z1pa/a9nn1cvr/svye1fVm5djvb+qTrvzlwkAAMB0hw3cqjomyauTPD7J7iTnVtXuHcNekeTi7n5EkguSXLhse78k5yf5jiRnJDm/qk5Y2+5p3f2o5fVny7JnJfl0dz8sySuT/MQdvjoAAACOGpvcwT0jyZ7uvqG7v5TkkiTn7BizO8kVy/Q719Y/Lsnl3X1zd386yeVJzj7M8c5J8vpl+q1JHltVtcF5AgAAcBTbJHBPTnLj2vzeZdm6a5I8ZZl+UpLjq+rEDbZ97fJ48v+5FrFf2aa7b0tyS5ITNzhPAAAAjmKbBO6B7p72jvkXJjmzqj6Q5MwkH0ty22G2fVp3PzzJdy2vp9+O46WqnltV21W1vW/fvsNfBQAAAKNtErh7k5y6Nn9KkpvWB3T3Td395O5+dJKXLMtuOdS23f2x5c9bk7wxq0ehv+p4VXVskvskuXnnSXX3a7p7q7u3du3atcFlAAAAMNkmgXtlktOr6qFVdVySpya5dH1AVZ1UVfv39eIkFy3TlyU5q6pOWL5c6qwkl1XVsVV10rLtvZL8gyQfXLa5NMn+b1v+3iS/191/5Q4uAAAArDv2cAO6+7aqel5WsXpMkou6+7qquiDJdndfmuQxSS6sqk7y7iQ/tGx7c1W9LKtITpILlmX/WVahe69ln7+b5OeXMb+Y5Jeqak9Wd26fehddKwAAAIPVhJujW1tbvb29faRPAwAAgLtBVV3V3VuHG7fJI8oAAABwjydwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGGGjwK2qs6vq+qraU1UvOsD6h1TVFVV1bVW9q6pOWVt3XlV9ZHmdtyz761X121X1H6rquqr68bXxz6yqfVV19fJ69l1xoQAAAMx22MCtqmOSvDrJ45PsTnJuVe3eMewVSS7u7kckuSDJhcu290tyfpLvSHJGkvOr6oT923T3tyR5dJL/pqoev7a/N3f3o5bXL9zxywMAAOBosckd3DOS7OnuG7r7S0kuSXLOjjG7k1yxTL9zbf3jklze3Td396eTXJ7k7O7+fHe/M0mWff5hklMCAAAAd9AmgXtykhvX5vcuy9Zdk+Qpy/STkhxfVSdusm1V3TfJ/5C/DOQkecqHZD8DAAAPYUlEQVTyuPNbq+rUDc4RAACAo9wmgVsHWNY75l+Y5Myq+kCSM5N8LMlth9u2qo5N8qYkr+ruG5bFv5nktOVx599N8voDnlTVc6tqu6q29+3bt8FlAAAAMNkmgbs3yfpd1FOS3LQ+oLtv6u4nd/ejk7xkWXbLBtu+JslHuvun1vb1qe7+4jL780m+7UAn1d2v6e6t7t7atWvXBpcBAADAZJsE7pVJTq+qh1bVcUmemuTS9QFVdVJV7d/Xi5NctExfluSsqjph+XKps5ZlqaofS3KfJP/rjn09aG32iUk+dPsuCQAAgKPRYQO3u29L8ryswvRDSd7S3ddV1QVV9cRl2GOSXF9VH07ygCQvX7a9OcnLsorkK5Nc0N03L79G6CVZfTnVH+74dUDPX3510DVJnp/kmXfNpQIAADBZde/8OO3Xn62trd7e3j7SpwEAAMDdoKqu6u6tw43b5BFlAAAAuMcTuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIywUeBW1dlVdX1V7amqFx1g/UOq6oqquraq3lVVp6ytO6+qPrK8zltb/m1V9UfLPl9VVbUsv19VXb6Mv7yqTrgrLhQAAIDZDhu4VXVMklcneXyS3UnOrardO4a9IsnF3f2IJBckuXDZ9n5Jzk/yHUnOSHL+WrD+bJLnJjl9eZ29LH9Rkiu6+/QkVyzzAAAAcEib3ME9I8me7r6hu7+U5JIk5+wYszurGE2Sd66tf1ySy7v75u7+dJLLk5xdVQ9K8s3d/b7u7iQXJ/meZZtzkrx+mX792nIAAAA4qE0C9+QkN67N712WrbsmyVOW6SclOb6qTjzEticv0wfa5wO6++NJsvx5/w3OEQAAgKPcJoFbB1jWO+ZfmOTMqvpAkjOTfCzJbYfYdpN9Hvqkqp5bVdtVtb1v377bsykAAAADbRK4e5OcujZ/SpKb1gd0903d/eTufnSSlyzLbjnEtnuX6QPt80+XR5iz/PlnBzqp7n5Nd29199auXbs2uAwAAAAm2yRwr0xyelU9tKqOS/LUJJeuD6iqk6pq/75enOSiZfqyJGdV1QnLl0udleSy5dHjW6vqby/fnvyMJL+xbHNpkv3ftnze2nIAAAA4qMMGbnffluR5WcXqh5K8pbuvq6oLquqJy7DHJLm+qj6c5AFJXr5se3OSl2UVyVcmuWBZliT/OMkvJNmT5KNJ3r4s//Ek311VH0ny3cs8AAAAHFKtvsT469vW1lZvb28f6dMAAADgblBVV3X31uHGbfKIMgAAANzjCVwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABGELgAAACMIHABAAAYQeACAAAwgsAFAABgBIELAADACAIXAACAEQQuAAAAIwhcAAAARhC4AAAAjCBwAQAAGEHgAgAAMILABQAAYASBCwAAwAgCFwAAgBEELgAAACMIXAAAAEYQuAAAAIwgcAEAABhB4AIAADCCwAUAAGAEgQsAAMAIAhcAAIARBC4AAAAjCFwAAABG2Chwq+rsqrq+qvZU1YsOsP5vVNU7q+oDVXVtVT1hWX5cVb22qv6oqq6pqscsy4+vqqvXXp+sqp9a1j2zqvatrXv2XXi9AAAADHXs4QZU1TFJXp3ku5PsTXJlVV3a3f9+bdiPJHlLd/9sVe1O8rYkpyV5TpJ098Or6v5J3l5V397dtyZ51Noxrkryq2v7e3N3P+/OXRoAAABHk03u4J6RZE9339DdX0pySZJzdozpJN+8TN8nyU3L9O4kVyRJd/9Zks8k2VrfsKpOT3L/JO+5IxcAAAAAyWaBe3KSG9fm9y7L1r00yff//+3dbYimVRkH8P/F7sKiZbhoVr5kkdTU0AsOQbBBam/0QRN72bEEYcIK20CEouaDVkgUlB+WIBTNCBqNrLAPUYgDOWjobGitLCQFitIHq8VIslY7fZhn9Nlp1pndzb3HM78fLPvc15775np2YM7855z7fqrqsSyt3u4e1R9MclFVba2q1yU5N8mZK86dztKKbRurXTLa6vzjqlo5HgCATWbHjh2pKn+qsmPHjqG/HLBhrblFOUmtUmsrjqeT3NJa+1ZVvSvJD6pqMsnNSSaSLCZ5JMk9SZ5Zce6uJJeNHf88yVxr7V9V9Zkk309y/v80VXVFkiuS5KyzzlrH2wAA4KXqwIEDOXQ9ZPOqWu3HcyBZX8B9LIeuup6R57cgL5tJ8sEkaa3dW1Xbk5wy2pZ81fKgqronycNjx29LsrW1tne51lr769h1b0zyjdWaaq3dkOSGJJmamvLdDgCgY+2ak5JrXzF0GxtCu+aktQfBJrWegHt/knNGW4wfz9KK66Urxjya5IIkt1TVRJLtSZ6oqhOSVGvtqap6X5JnVjycajrJ3PiFqurVrbU/jw4vTLL/SN8UAAB9qa/83QruSFWlXTt0F7AxrRlwW2vPVNXnkvwyyZYkN7fWHqqqryZZbK3dkeTqJDdW1VVZ2r58eWutjZ6c/Muq+k+WwvFlKy7/sSQfWlH7fFVdmKWtzH9LcvnRvz0AAAA2i+rhN2FTU1NtcXFx6DYAAHiRVJUV3BH/F2xGVbW3tTa11rj1PEUZAAAANjwBFwAAgC4IuAAAAHRBwAUAAKALAi4AAABdWM/n4AIAwOCqaugWNoSTTz556BZgwxJwAQDY8HwsDrAetigDAADQBQEXAACALgi4AAAAdEHABQAAoAsCLgAAAF0QcAEAAOiCgAsAAEAXBFwAAAC6IOACAADQBQEXAACALgi4AAAAdEHABQAAoAsCLgAAAF0QcAEAAOiCgAsAAGuYm5vL5ORktmzZksnJyczNzQ3dErCKrUM3AAAAG9nc3FxmZ2dz0003ZefOnVlYWMjMzEySZHp6euDugHHVWhu6h2M2NTXVFhcXh24DAIAOTU5OZs+ePTnvvPOeq83Pz2f37t3Zt2/fgJ3B5lFVe1trU2uOE3ABAODwtmzZkqeffjrbtm17rnbw4MFs3749zz777ICdweax3oDrHlwAAHgBExMTWVhYOKS2sLCQiYmJgToCDkfABQCAFzA7O5uZmZnMz8/n4MGDmZ+fz8zMTGZnZ4duDVjBQ6YAAOAFLD9Iavfu3dm/f38mJiZy3XXXecAUbEDuwQUAAGBDcw8uAAAAm4qACwAAQBcEXAAAALog4AIAANAFARcAAIAuCLgAAAB0QcAFAACgCwIuAAAAXRBwAQAA6IKACwAAQBcEXAAAALog4AIAANAFARcAAIAuCLgAAAB0QcAFAACgCwIuAAAAXRBwAQAA6IKACwAAQBcEXAAAALog4AIAANAFARcAAIAuCLgAAAB0QcAFAACgCwIuAAAAXRBwAQAA6EK11obu4ZhV1RNJHhm6D9iETknyl6GbAIDjyNwHw3hta+3UtQZ1EXCBYVTVYmttaug+AOB4MffBxmaLMgAAAF0QcAEAAOiCgAscixuGbgAAjjNzH2xg7sEFAACgC1ZwAQAA6IKAC6yqqv6xSu3aqnq8qh6oqoer6idV9eYVY06tqoNV9enj1y0AHJuqOq2qflhVf6qqvVV1b1VdXFXvqaonR3Pf76rqzqp65eicy6uqVdUFY9e5eFT7yHDvBjYvARc4Ute31t7eWjsnyW1J7qqq8c8k+2iS3ySZHqQ7ADhCVVVJfpbk162117fWzk2yK8kZoyF3j+a+tya5P8mVY6f/PofOebuSPHgc2gZWIeACR621dluSXyW5dKw8neTqJGdU1emDNAYAR+b8JP9urX13udBae6S1tmd80CgIvzzJgbHy3UneWVXbquplSd6Q5IHj0DOwCgEXOFa/TfKmJKmqM5O8qrV2X5IfJfn4kI0BwDq9JUvz2eG8u6oeSPJokvcmuXns31qSO5N8IMlFSe54sZoE1ibgAseqxl7vylKwTZJbY5syAC9BVfWdqnqwqu4flZa3KJ+Z5HtJvrnilFuzNAfuSjJ3HFsFVtg6dAPAS947kiyOXk8nOa2qPjE6fk1VndNae3iY1gBgXR5KcsnyQWvtyqo6Jc/Pb+PuSHL7eKG1dl9VTSb5Z2vtD0s7mYEhWMEFjlpVXZLk/UnmquqNSU5srZ3eWju7tXZ2kq9n6bfZALCR3ZVke1V9dqx2wmHG7kzyx1XqX0ry5f93Y8CRsYILHM4JVfXY2PG3R39fVVWfTHJikn1Jzm+tPVFVVyb56Ypr3J6lbVtfe9G7BYCj1FprVfXhJNdX1ReSPJHkqSRfHA1Zvge3kjyZ5FOrXOMXx6tf4PCqtTZ0DwAAAHDMbFEGAACgCwIuAAAAXRBwAQAA6IKACwAAQBcEXAAAALog4AIAANAFARcAAIAuCLgAAAB04b+cne/Mhm29fwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison - Post Tuning')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Finalize Model\n",
    "Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:\n",
    "* Using an optimal model tuned to make predictions on unseen data.\n",
    "* Creating a standalone model using the tuned parameters\n",
    "* Saving an optimal model to file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"Model Validation and Final Model Creation has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.a) Predictions on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9142947501581278\n",
      "[[491   5   0   0   0   0   0   0   0   0   0   0]\n",
      " [  5 466   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1  28 391   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 355 150   0   3   0   0   0   0   0]\n",
      " [  0   0   0  32 524   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 545   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  23   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  10   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0  24   0   7   0]\n",
      " [  0   0   0   0   0   0   1   0   0  12   0  12]\n",
      " [  2   0   0   0   1   0  10   0   8   0  28   0]\n",
      " [  0   0   0   0   0   0   1   0   0   3   1  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.99      0.99       496\n",
      "           2       0.93      0.99      0.96       471\n",
      "           3       1.00      0.93      0.96       420\n",
      "           4       0.92      0.70      0.79       508\n",
      "           5       0.78      0.94      0.85       556\n",
      "           6       1.00      1.00      1.00       545\n",
      "           7       0.59      1.00      0.74        23\n",
      "           8       1.00      1.00      1.00        10\n",
      "           9       0.75      0.75      0.75        32\n",
      "          10       0.80      0.48      0.60        25\n",
      "          11       0.78      0.57      0.66        49\n",
      "          12       0.65      0.81      0.72        27\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      3162\n",
      "   macro avg       0.85      0.85      0.84      3162\n",
      "weighted avg       0.92      0.91      0.91      3162\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.b) Create standalone model on entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_complete.shape: (10929, 300) y_complete.shape: (10929,)\n",
      "Model training time: 0:00:00.640544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_hhds7y1\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Combining the training and testing datasets to form the complete dataset that will be used for training the final model\n",
    "x_complete = np.vstack((x_train, x_test))\n",
    "y_complete = np.concatenate((y_train, y_test))\n",
    "print(\"x_complete.shape: {} y_complete.shape: {}\".format(x_complete.shape, y_complete.shape))\n",
    "\n",
    "finalModel = LinearDiscriminantAnalysis()\n",
    "finalModel.fit(x_complete, y_complete)\n",
    "print ('Model training time:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.c) Save model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelName = 'finalModel_BinaryClass.sav'\n",
    "# dump(finalModel, modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 4:57:22.592444\n"
     ]
    }
   ],
   "source": [
    "email_notify(\"Model Validation and Final Model Creation completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
