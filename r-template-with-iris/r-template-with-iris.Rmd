---
title: "R Project Template with the Iris Dataset"
author: "David Lowe"
date: "July 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R Project Template
Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)

Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.

Any predictive modeling machine learning project can be broken down into about 6 common tasks:

1. Define Problem
2. Summarize Data
3. Prepare Data
4. Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(caret)
```

### 1.b) Load dataset
Although R has the Iris dataset built-in, we will download the Iris dataset from the UCI Machine Learning Repository. The dataset has no header, so we will add the column header after loading the dataset.

```{r}
filename <- "iris.data"
dataset <- read.csv(filename, header=FALSE)
colnames(dataset) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species")
```

### 1.c) Split-out validation dataset
Create a training (variable name "dataset") and a validation (variable name "validation") dataset

```{r}
set.seed(7)
# create a list of 80% of the rows in the original dataset we can use for training
training_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-training_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[training_index,]
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

### 2.a) Descriptive statistics

#### 2.a.i) Dimensions of the dataset.

```{r}
dim(dataset)
```

#### 2.a.ii) Types of the attributes.

```{r}
sapply(dataset, class)
```

#### 2.a.iii) Peek at the data itself.

```{r}
head(dataset)
tail(dataset)
```

#### 2.a.iv) Levels of the class attribute.

```{r}
levels(dataset$Species)
```

#### 2.a.v) Breakdown of the instances in each class.

```{r}
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
```

#### 2.a.vi) Statistical summary of all attributes.

```{r}
summary(dataset)
```

### 2.b) Data visualizations

#### 2.b.i) Univariate plots to better understand each attribute.

```{r}
x <- dataset[,1:4]
y <- dataset[,5]
par(mfrow=c(1,4))
  for(i in 1:4) {
  boxplot(x[,i], main=names(iris)[i])
  }
```

```{r}
plot(y)
```

#### 2.b.ii) Multivariate plots to better understand the relationships between attributes

```{r}
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
```

```{r}
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
```

```{r}
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
```

## 3. Prepare Data
Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

For the Iris example dataset, no additional data-prep tasks are required since it is a very simple and straight-forward data set. The sub-sections and headings are left intact as an illustration.

### 3.a) Data Cleaning

### 3.b) Feature Selection

### 3.c) Data Transforms

## 4. Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the dataset. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For the Iris example, we will evaluate 5 different algorithms:

Linear Discriminant Analysis (LDA)
Classification and Regression Trees (CART).
k-Nearest Neighbors (kNN).
Support Vector Machines (SVM) with a linear kernel.
Random Forest (RF)

This is a good mixture of simple linear (LDA), nonlinear (CART, kNN) and complex nonlinear methods (SVM, RF). The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

### 4.b) Spot Check Algorithms

```{r}
# a) linear algorithms
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)
```

### 4.c) Compare Algorithms

```{r}
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
```

```{r}
# compare accuracy of models
dotplot(results)
```

```{r}
# summarize Best Model (LDA in this case)
print(fit.lda)
```

## 5. Improve Accuracy
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models:

* Search for a combination of parameters for each algorithm using caret that yields the best results.
* Combine the prediction of multiple models into an ensemble prediction using standalone algorithms or the caretEnsemble package.

For the Iris example dataset, no additional tuning tasks are required since it is a very simple and straight-forward data set. The sub-sections and headings are left intact as an illustration.

### 5.a) Algorithm Tuning

### 5.b) Ensembles

## 6. Finalize Model
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:

* Using an optimal model tuned to make predictions on unseen data.
* Creating a standalone model using the tuned parameters
* Saving an optimal model to file for later use.

### 6.a) Predictions on validation dataset

```{r}
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Species)
```

### 6.b) Create standalone model on entire training dataset

### 6.c) Save model for later use
