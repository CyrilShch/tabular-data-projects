---
title: "Multi-Class Classification Model for Letter Recognition Using R"
author: "David Lowe"
date: "August 24, 2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [http://machinelearningmastery.com/]

SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Letter Recognition Data Set is a multi-class classification situation where we are trying to predict one of the several possible outcomes.

INTRODUCTION: The objective is to identify each of many black-and-white rectangular-pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.

CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 79.30%. Three algorithms (Bagged CART, Random Forest, and k-Nearest Neighbors) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 96.32%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 96.45%, which was even slightly better the accuracy of the training data.

For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.

Dataset Used: Letter Recognition

Dataset ML Model: Multi-class classification with numerical attributes

Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition

One potential source of performance benchmarks: https://www.kaggle.com/c/ci-letter-recognition

The project aims to touch on the following areas:

1. Document a predictive modeling problem end-to-end.
2. Explore data cleaning and transformation options
3. Explore non-ensemble and ensemble algorithms for baseline model performance
4. Explore algorithm tuning techniques for improving model performance

Any predictive modeling machine learning project genrally can be broken down into about six major tasks:

1. Prepare Problem
2. Summarize Data
3. Prepare Data
4. Model and Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(caret)
library(corrplot)
```

### 1.b) Load dataset

```{r}
startTimeScript <- proc.time()
entireDataset <- read.csv("letter-recognition.data", header= F, stringsAsFactors = T)

# Rename the class column/target to a standard label
colnames(entireDataset) <- c("targetVar","x_box","y_box","width","high","onpix","x_bar","y_bar","x2bar","y2bar","xybar","x2ybr","xy2br","x_ege","xegvy","y_ege","yegvx")
```

```{r}
# Setting up various variables to control the upcoming data visualization charts
# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(entireDataset)

# Set up a variable for the total number of attribute columns (totAttr)
totAttr <- totCol-1

#Which column does the first attribute column begin (1 - First, 2 - second)?
beginAttrCol <- 2
if (beginAttrCol == 1) {
attrColOffset <- 0
} else {
attrColOffset <- 1
}

# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr
dispCol <- 4
if (totAttr%%dispCol == 0) {
dispRow <- totAttr%/%dispCol
} else {
dispRow <- (totAttr%/%dispCol) + 1
}
cat("Will attempt to create graphics grid (col x row): ", dispCol, ' by ', dispRow)
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

### 2.a) Descriptive statistics

#### 2.a.i) Peek at the data itself.

```{r}
head(entireDataset)
```

#### 2.a.ii) Dimensions of the dataset.

```{r}
dim(entireDataset)
```

#### 2.a.iii) Types of the attributes.

```{r}
sapply(entireDataset, class)
```

#### 2.a.iv) Statistical summary of all attributes.

```{r}
summary(entireDataset)
```

#### 2.a.v) Summarize the levels of the class attribute.

```{r}
entireDataset_x <- entireDataset[,beginAttrCol:(totAttr+attrColOffset)]
if (beginAttrCol == 1) {
entireDataset_y <- entireDataset[,totCol]
} else {
entireDataset_y <- entireDataset[,1]
}
cbind(freq=table(entireDataset_y), percentage=prop.table(table(entireDataset_y))*100)
```

#### 2.a.vi) Count missing values.

```{r}
sapply(entireDataset, function(x) sum(is.na(x)))
```

### 2.b) Data visualizations

#### 2.b.i) Univariate plots to better understand each attribute.

```{r}
# Boxplots for each attribute
par(mfrow=c(dispRow,dispCol))
for(i in beginAttrCol:(totAttr+attrColOffset)) {
	boxplot(entireDataset[,i], main=names(entireDataset)[i])
}
```

```{r}
# Histograms each attribute
par(mfrow=c(dispRow,dispCol))
for(i in beginAttrCol:(totAttr+attrColOffset)) {
	hist(entireDataset[,i], main=names(entireDataset)[i])
}
```

```{r}
# Density plot for each attribute
par(mfrow=c(dispRow,dispCol))
for(i in beginAttrCol:(totAttr+attrColOffset)) {
	plot(density(entireDataset[,i]), main=names(entireDataset)[i])
}
```

#### 2.b.ii) Multivariate plots to better understand the relationships between attributes

```{r}
# Scatterplot matrix colored by class
pairs(targetVar~., data=entireDataset, col=entireDataset$targetVar)
```

```{r}
# Box and whisker plots for each attribute by class
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=entireDataset_x, y=entireDataset_y, plot="box", scales=scales)
```

```{r}
# Density plots for each attribute by class value
featurePlot(x=entireDataset_x, y=entireDataset_y, plot="density", scales=scales)
```

```{r}
# Correlation plot
correlations <- cor(entireDataset_x)
corrplot(correlations, method="circle")
```

## 3. Prepare Data
Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

### 3.a) Data Cleaning

```{r}
# Not applicable for this iteration of the project.

# Mark missing values
#invalid <- 0
#entireDataset$some_col[entireDataset$some_col==invalid] <- NA

# Impute missing values
#entireDataset$some_col <- with(entireDataset, impute(some_col, mean))
```

### 3.b) Feature Selection

```{r}
# Not applicable for this iteration of the project.
```

### 3.c) Data Transforms

```{r}
# Not applicable for this iteration of the project.
```

### 3.d) Split-out training and validation datasets
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
seedNum <- 888
set.seed(seedNum)

# Create a list of the rows in the original dataset we can use for training
# According to the dataset authors, they took the first 16,000 items for training the model,
# and the last 4,000 items for testing the model. This project will do the same.
#training_index <- createDataPartition(entireDataset$targetVar, p=0.8, list=FALSE)
n <- nrow(entireDataset)
#entireDataset <- entireDataset[sample(n),]
training_indices <- 1:round(0.8 * n)
validation_indices <- (round(0.8 * n) + 1):n

# Use 80% of data to training and testing the models
training <- entireDataset[training_indices,]

# Select the remaining 20% of the data for validation
validation <- entireDataset[validation_indices,]
```

```{r}
proc.time()-startTimeScript
```

## 4. Model and Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the training. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For this project, we will evaluate one linear, three non-linear, and two ensemble algorithms:

Linear Algorithm: Linear Discriminant Analysis

Non-Linear Algorithms: Decision Trees (CART), k-Nearest Neighbors, and Support Vector Machine

Ensemble Algorithms: Bagged CART and Random Forest

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Set test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metricTarget <- "Accuracy"
```

### 4.b) Generate models using linear algorithms

```{r LDA}
# Linear Discriminant Analysis (Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.lda <- train(targetVar~., data=training, method="lda", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

### 4.c) Generate models using nonlinear algorithms

```{r CART}
# Decision Tree - CART (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.cart <- train(targetVar~., data=training, method="rpart", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r KNN}
# k-Nearest Neighbors (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.knn <- train(targetVar~., data=training, method="knn", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r SVM}
# Support Vector Machine (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.svm <- train(targetVar~., data=training, method="svmRadial", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

### 4.d) Generate models using ensemble algorithms
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r BAGCART}
# Bagged CART (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.bagcart <- train(targetVar~., data=training, method="treebag", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r RF}
# Random Forest (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.rf <- train(targetVar~., data=training, method="rf", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

### 4.e) Compare baseline algorithms

```{r SPOT_CHECK}
results <- resamples(list(LDA=fit.lda, CART=fit.cart, kNN=fit.knn, SVM=fit.svm, BagCART=fit.bagcart, RF=fit.rf))
summary(results)
dotplot(results)
```

## 5. Improve Accuracy or Results
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models.

Using the three best-perfoming algorithms from the previous section, we will Search for a combination of parameters for each algorithm that yields the best results.

### 5.a) Algorithm Tuning
Finally, we will tune the best-performing algorithms from each group further and see whether we can get more accuracy out of them.

```{r FINAL1}
# Tuning algorithm #1 - Bagged CART
startTimeModule <- proc.time()
set.seed(seedNum)
fit.final1 <- fit.bagcart
print(fit.final1)
proc.time()-startTimeModule
```

```{r FINAL2}
# Tuning algorithm #2 - Random Forest
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(mtry = c(1:5))
fit.final2 <- train(targetVar~., data=training, method="rf", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.final2)
print(fit.final2)
proc.time()-startTimeModule
```

```{r FINAL3}
# Tuning algorithm #3 - k-Nearest Neighbors
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(k = c(3,5,7))
fit.final3 <- train(targetVar~., data=training, method="knn", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.final3)
print(fit.final3)
proc.time()-startTimeModule
```

### 5.d) Compare Algorithms After Tuning

```{r POST_TUNING}
results <- resamples(list(BAGCART=fit.final1, RF=fit.final2, KNN=fit.final3))
summary(results)
dotplot(results)
```

## 6. Finalize Model and Present Results
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:

* Using an optimal model tuned to make predictions on unseen data.
* Creating a standalone model using the tuned parameters
* Saving an optimal model to file for later use.

### 6.a) Predictions on validation dataset

```{r PREDICT}
predictions <- predict(fit.final2, newdata=validation)
confusionMatrix(predictions, validation$targetVar)
```

### 6.b) Create standalone model on entire training dataset

```{r}
library(randomForest)
startTimeModule <- proc.time()
set.seed(seedNum)
finalModel <- randomForest(targetVar~., entireDataset, mtry=3, na.action=na.omit)
summary(finalModel)
proc.time()-startTimeModule
```

### 6.c) Save model for later use

```{r}
#saveRDS(finalModel, "./finalModel_MultiClass.rds")
```

```{r}
proc.time()-startTimeScript
```
