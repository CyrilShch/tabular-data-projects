---
title: "Algorithm Test Model for [Algorithm Name] Using R"
author: "David Lowe"
date: "April 13, 2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)

Dataset Used: [Dataset Name]

Dataset ML Model: [Regression | Binary classification | Multi-classification] with [numerical | categorical] attributes

Dataset Reference: [Dataset URL]

For performance benchmarks, please consult: [Benchmark URL]

The purpose of this project is to analyze predictions using one machine learning algorithm and to document the steps using a template.

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(mlbench)
library(caret)
#library(corrplot)
library(ROCR)
```

### 1.b) Load dataset

```{r}
#entireDataset <- read.csv2("bank-partial.csv", header= TRUE)
data(PimaIndiansDiabetes)
entireDataset <- PimaIndiansDiabetes

# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(entireDataset)

# Rename the class/target column to a standard label
colnames(entireDataset)[ncol(entireDataset)] <- "targetVar"
```

## 2. Prepare Data
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
seedNum <- 888
set.seed(seedNum)

# Create a list of 80% of the rows in the original dataset we can use for training
training_index <- createDataPartition(entireDataset$targetVar, p=0.80, list=FALSE)

# Use 80% of data to training and testing the models
training <- entireDataset[training_index,]

# Select the remaining 20% of the data for validation
validation <- entireDataset[-training_index,]
```

## 4. Model and Evaluate Algorithms
For this project, we will evaluate one of the following linear, non-linear, and ensemble algorithms:

Linear Algorithms: Linear Regression, Logistic Regression and Linear Discriminant Analysis

Non-Linear Algorithms: Decision Trees (CART), Naive Bayes, k-Nearest Neighbors, and Support Vector Machine

Ensemble Algorithms: Bagged CART, Random Forest, Adaboost, and Stochastic Gradient Boosting

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Set test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metricTarget <- "Accuracy"
```

### 4.b) Generate models using algorithms

```{r ADABOOST1}
# AdaBoost (Classification)
set.seed(seedNum)
fit.ada1 <- train(targetVar~., data=training, method="adaboost", metric=metricTarget, trControl=control)
```

```{r ADABOOST2}
# AdaBoost (Classification)
set.seed(seedNum)
fit.ada2 <- train(targetVar~., data=training, method="AdaBoost.M1", metric=metricTarget, trControl=control)
```

```{r ADABOOST3}
# AdaBoost (Classification)
set.seed(seedNum)
fit.ada3 <- train(targetVar~., data=training, method="AdaBag", metric=metricTarget, trControl=control)
```

```{r ADABOOST4}
# AdaBoost (Classification)
set.seed(seedNum)
fit.ada4 <- train(targetVar~., data=training, method="ada", metric=metricTarget, trControl=control)
```

### 4.e) Compare baseline algorithms

```{r SPOT_CHECK}
results <- resamples(list(ADABOOST=fit.ada1, ADABOOSTM1=fit.ada2, ADABAG=fit.ada3, ADA=fit.ada4))
summary(results)
dotplot(results)
```

## 5. Improve Accuracy or Results
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models:

* Search for a combination of parameters for each algorithm using caret that yields the best results.
* Combine the prediction of multiple models into an ensemble prediction using standalone algorithms or the caret Ensemble package.

### 5.a) Algorithm Tuning
Finally, we will tune the best-performing algorithms from each group further and see whether we can get more accuracy out of them.

```{r tuning1}
# Tuning algorithm #1 - Support Vector Machine
set.seed(seedNum)
#grid <- expand.grid(sigma = c(.01, .015, 0.2), C = c(0.25, 0.5, 0.75, 1))
#fit.tuning1 <- train(targetVar~., data=training, method="svmRadial", metric=metricTarget, tuneGrid=grid, trControl=control)
#plot(fit.tuning1)
#print(fit.tuning1)
```

```{r tuning2}
# Tuning algorithm #2 - Random Forest
set.seed(seedNum)
#grid <- expand.grid(mtry = c(1:5))
#fit.tuning2 <- train(targetVar~., data=training, method="rf", metric=metricTarget, tuneGrid=grid, trControl=control)
#plot(fit.tuning2)
#print(fit.tuning2)
```

```{r tuning3}
# Tuning algorithm #3 - Stochastic Gradient Boosting
set.seed(seedNum)
#grid <- expand.grid(.n.trees=c(100, 200, 300), .shrinkage=c(0.01, 0.05, 0.1), .interaction.depth=c(1, 2, 3), .n.minobsinnode=c(3, 6, 10))
#fit.tuning3 <- train(targetVar~., data=training, method="gbm", metric=metricTarget, tuneGrid=grid, trControl=control)
#plot(fit.tuning3)
#print(fit.tuning3)
```

### 5.d) Compare Algorithms After Tuning

```{r POST_TUNING}
#results <- resamples(list(LogReg=fit.tuning1, SVM=fit.tuning2, GBM=fit.tuning3))
#summary(results)
#dotplot(results)
```

## 6. Finalize Model and Present Results
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it.

```{r PREDICT}
#predictions <- predict(fit.tuning1, newdata=validation)

# The next steps are for regression problems
#valY <- validation[,totCol]
#print(RMSE(predictions, valY))
#print(R2(predictions, valY))

# The next step is for both binary-class and multi-class problems
#confusionMatrix(predictions, validation$targetVar)

# The next steps are for only binary class problems
#pred <- prediction(as.numeric(predictions), as.numeric(validation$targetVar))
#perf <- performance(pred, measure = "tpr", x.measure = "fpr")
#plot(perf, colorize=TRUE)
#auc <- performance(pred, measure = "auc")
#auc <- auc@y.values[[1]]
#auc
```
