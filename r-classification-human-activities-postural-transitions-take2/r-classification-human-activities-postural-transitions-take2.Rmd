---
title: "Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 1"
author: "David Lowe"
date: "February 1, 2019"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]

SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities and Postural Transitions dataset is a classic multi-class classification situation where we are trying to predict one of the 12 possible outcomes.

INTRODUCTION: The research team carried out experiments with a group of 30 volunteers who performed a protocol of activities composed of six basic activities. There are three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs). The experiment also included postural transitions that occurred between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a smartphone on the waist during the experiment execution. The research team also video-recorded the activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and 30% for the testing.

In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Because the dataset has many attributes that were collinear with other attributes, we eliminated the attributes that have a collinearity measurement of 99% or higher. Iteration Take1 established the performance baseline for accuracy and processing time.

In the current iteration Take2, we will examine the feature selection technique of eliminating collinear features. We will perform iterative modeling at collinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear features, we hope to decrease the processing time and maintain a comparable level of model accuracy comparing to iteration Take1.

ANALYSIS: In iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.61%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 97.70%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 92.85%, which was below the training data and possibly due to over-fitting.

From the model-building perspective, the number of attributes decreased by 108, from 561 down to 453.

COL_75%: In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.75%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 97.84%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 94.75%, which was below the training data and possibly due to over-fitting.

From the model-building perspective, the number of attributes decreased by 408, from 561 down to 153. The processing time went from 10 hours 17 minutes in iteration Take1 down to 4 hours 26 minutes in Take2, which was a reduction of 66.6%.

COL_80%: In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.27%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 98.00%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 94.91%, which was below the training data and possibly due to over-fitting.

From the model-building perspective, the number of attributes decreased by 385, from 561 down to 176. The processing time went from 10 hours 17 minutes in iteration Take1 down to 4 hours 56 minutes in Take2, which was a reduction of 47.9%.

COL_85%: In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.31%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 98.14%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 94.15%, which was below the training data and possibly due to over-fitting.

From the model-building perspective, the number of attributes decreased by 362, from 561 down to 199. The processing time went from 10 hours 17 minutes in iteration Take1 down to 5 hours 27 minutes in Take2, which was a reduction of 47.0%.

COL_90%: In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.38%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 98.17%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 94.12%, which was below the training data and possibly due to over-fitting.

From the model-building perspective, the number of attributes decreased by 338, from 561 down to 223. The processing time went from 10 hours 17 minutes in iteration Take1 down to 6 hours 2 minutes in Take2, which was a reduction of 41.3%.

COL_95%: In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.48%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 98.31%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 94.97%, which was below the training data and possibly due to over-fitting.

From the model-building perspective, the number of attributes decreased by 278, from 561 down to 283. The processing time went from 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in Take2, which was a reduction of 23.9%.

CONCLUSION: For this iteration, eliminating collinear features at the 95% level and using the eXtreme Gradient Boosting algorithm achieved the best overall results. For this dataset, we should consider using the eXtreme Gradient Boosting algorithm for further modeling or production use.

Dataset Used: Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set

Dataset ML Model: Multi-class classification with numerical attributes

Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions

The project aims to touch on the following areas:

1. Document a predictive modeling problem end-to-end.
2. Explore data cleaning and transformation options
3. Explore non-ensemble and ensemble algorithms for baseline model performance
4. Explore algorithm tuning techniques for improving model performance

Any predictive modeling machine learning project genrally can be broken down into about six major tasks:

1. Prepare Problem
2. Summarize Data
3. Prepare Data
4. Model and Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
startTimeScript <- proc.time()
library(caret)
library(corrplot)
library(mailR)
library(stringr)
library(MLmetrics)
library(DMwR)
library(mlbench)

# Create one random seed number for reproducible results
seedNum <- 888
set.seed(seedNum)
```

### 1.b) Set up the email notification function

```{r}
email_notify <- function(msg=""){
  sender <- "luozhi2488@gmail.com"
  receiver <- "dave@contactdavidlowe.com"
  sbj_line <- "Notification from R Multi-Class Classification Script"
  password <- readLines("../email_credential.txt")
  send.mail(
    from = sender,
    to = receiver,
    subject= sbj_line,
    body = msg,
    smtp = list(host.name = "smtp.gmail.com", port = 465, user.name = sender, passwd = password, ssl = TRUE),
    authenticate = TRUE,
    send = TRUE)
}
```

```{r}
email_notify(paste("Library and Data Loading has begun!",date()))
```

### 1.c) Load dataset

```{r}
widthVector <- rep(16, 561)
colNames <- paste0("attr",1:561)
x_train <- read.csv("X_train.txt", header = FALSE, col.names = colNames, sep = " ")
y_train <- read.csv("Y_train.txt", header = FALSE, col.names = c("targetVar"))
y_train$targetVar <- as.factor(y_train$targetVar)
xy_train <- cbind(x_train, y_train)

x_test <- read.csv("X_test.txt", header = FALSE, col.names = colNames, sep = " ")
y_test <- read.csv("Y_test.txt", header = FALSE, col.names = c("targetVar"))
y_test$targetVar <- as.factor(y_test$targetVar)
xy_test <- cbind(x_test, y_test)
```


```{r}
# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(xy_train)

# Set up variable totAttr for the total number of attribute columns
totAttr <- totCol-1
```

### 1.d) Set up the key parameters to be used in the script

```{r}
# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr
dispCol <- 5
if (totAttr%%dispCol == 0) {
dispRow <- totAttr%/%dispCol
} else {
dispRow <- (totAttr%/%dispCol) + 1
}
cat("Will attempt to create graphics grid (col x row): ", dispCol, ' by ', dispRow)
```

### 1.e) Set test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=1)
metricTarget <- "Accuracy"
```

```{r}
email_notify(paste("Library and Data Loading completed!",date()))
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

```{r}
email_notify(paste("Data Summarization and Visualization has begun!",date()))
```

### 2.a) Descriptive statistics

#### 2.a.i) Peek at the data itself.

```{r}
head(xy_train)
```

#### 2.a.ii) Dimensions of the dataset.

```{r}
dim(xy_train)
```

#### 2.a.iii) Types of the attributes.

```{r}
sapply(xy_train, class)
```

#### 2.a.iv) Statistical summary of all attributes.

```{r}
summary(xy_train)
```

#### 2.a.v) Count missing values.

```{r}
sapply(xy_train, function(x) sum(is.na(x)))
```

#### 2.a.vi) Summarize the levels of the class attribute.

```{r}
cbind(freq=table(y_train), percentage=prop.table(table(y_train))*100)
```

### 2.b) Data visualizations

#### 2.b.i) Univariate plots to better understand each attribute.

```{r}
# Boxplots for each attribute
# par(mfrow=c(dispRow,dispCol))
for(i in 1:totAttr) {
	boxplot(x_train[,i], main=names(x_train)[i])
}
```

```{r}
# Histograms each attribute
# par(mfrow=c(dispRow,dispCol))
for(i in 1:totAttr) {
	hist(x_train[,i], main=names(x_train)[i])
}
```

```{r}
# Density plot for each attribute
# par(mfrow=c(dispRow,dispCol))
for(i in 1:totAttr) {
	plot(density(x_train[,i]), main=names(x_train)[i])
}
```

#### 2.b.ii) Multivariate plots to better understand the relationships between attributes

```{r}
# Scatterplot matrix colored by class
# pairs(targetVar~., data=xy_train, col=xy_train$targetVar)
```

```{r}
# Box and whisker plots for each attribute by class
# scales <- list(x=list(relation="free"), y=list(relation="free"))
# featurePlot(x=x_train, y=y_train, plot="box", scales=scales)
```

```{r}
# Density plots for each attribute by class value
# featurePlot(x=x_train, y=y_train, plot="density", scales=scales)
```

```{r}
# Correlation plot
correlations <- cor(x_train)
corrplot(correlations, method="circle")
```

```{r}
email_notify(paste("Data Summarization and Visualization completed!",date()))
```

## 3. Prepare Data
Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

```{r}
email_notify(paste("Data Cleaning and Transformation has begun!",date()))
```

### 3.a) Data Cleaning

```{r}
# Not applicable for this iteration of the project
```

### 3.b) Feature Selection

```{r}
# Using the correlations calculated previously, we try to find attributes that are highly correlated.
highlyCorrelated <- findCorrelation(correlations, cutoff=0.95)
print(highlyCorrelated)
cat('Number of attributes found to be highly correlated:',length(highlyCorrelated))
```

```{r}
# Removing the highly correlated attributes from the training and validation dataframes
xy_train <- xy_train[, -highlyCorrelated]
xy_test <- xy_test[, -highlyCorrelated]
```

### 3.c) Data Transforms

```{r}
# Not applicable for this iteration of the project
```

### 3.d) Display the Final Dataset for Model-Building

```{r}
dim(xy_train)
dim(xy_test)
```

```{r}
sapply(xy_train, class)
```

```{r}
email_notify(paste("Data Cleaning and Transformation completed!",date()))
proc.time()-startTimeScript
```

## 4. Model and Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the training. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For this project, we will evaluate one linear, three non-linear, and three ensemble algorithms:

Linear Algorithm: Linear Discriminant Analysis

Non-Linear Algorithms: Decision Trees (CART) and k-Nearest Neighbors

Ensemble Algorithms: Bagged CART, Random Forest, and Stochastic Gradient Boosting

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Generate models using linear algorithms

```{r LDA}
# Linear Discriminant Analysis (Classification)
email_notify(paste("Linear Discriminant Analysis modeling has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
fit.lda <- train(targetVar~., data=xy_train, method="lda", metric=metricTarget, trControl=control)
print(fit.lda)
proc.time()-startTimeModule
email_notify(paste("Linear Discriminant Analysis modeling completed!",date()))
```

### 4.b) Generate models using nonlinear algorithms

```{r CART}
# Decision Tree - CART (Regression/Classification)
email_notify(paste("Decision Tree modeling has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
fit.cart <- train(targetVar~., data=xy_train, method="rpart", metric=metricTarget, trControl=control)
print(fit.cart)
proc.time()-startTimeModule
email_notify(paste("Decision Tree modeling completed!",date()))
```

```{r KNN}
# k-Nearest Neighbors (Regression/Classification)
email_notify(paste("k-Nearest Neighbors modeling has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
fit.knn <- train(targetVar~., data=xy_train, method="knn", metric=metricTarget, trControl=control)
print(fit.knn)
proc.time()-startTimeModule
email_notify(paste("k-Nearest Neighbors modeling completed!",date()))
```

### 4.c) Generate models using ensemble algorithms
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r BAGCART}
# Bagged CART (Regression/Classification)
email_notify(paste("Bagged CART modeling has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
fit.bagcart <- train(targetVar~., data=xy_train, method="treebag", metric=metricTarget, trControl=control)
print(fit.bagcart)
proc.time()-startTimeModule
email_notify(paste("Bagged CART modeling completed!",date()))
```

```{r RF}
# Random Forest (Regression/Classification)
email_notify(paste("Random Forest modeling has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
fit.rf <- train(targetVar~., data=xy_train, method="rf", metric=metricTarget, trControl=control)
print(fit.rf)
proc.time()-startTimeModule
email_notify(paste("Random Forest modeling completed!",date()))
```

```{r GBM}
# eXtreme Gradient Boosting (Regression/Classification)
email_notify(paste("eXTreme Gradient Boosting modeling has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
fit.xgb <- train(targetVar~., data=xy_train, method="xgbTree", metric=metricTarget, trControl=control, verbose=F)
print(fit.xgb)
proc.time()-startTimeModule
email_notify(paste("eXtreme Gradient Boosting modeling completed!",date()))
```

### 4.d) Compare baseline algorithms

```{r SPOT_CHECK}
results <- resamples(list(LDA=fit.lda, CART=fit.cart, kNN=fit.knn, BagCART=fit.bagcart, RF=fit.rf, XGB=fit.xgb))
summary(results)
dotplot(results)
cat('The average accuracy from all models is:',
    mean(c(results$values$`LDA~Accuracy`,results$values$`CART~Accuracy`,results$values$`kNN~Accuracy`,results$values$`BagCART~Accuracy`,results$values$`RF~Accuracy`,results$values$`XGB~Accuracy`)))
```

## 5. Improve Accuracy or Results
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models.

Using the three best-perfoming algorithms from the previous section, we will Search for a combination of parameters for each algorithm that yields the best results.

### 5.a) Algorithm Tuning
Finally, we will tune the best-performing algorithms from each group further and see whether we can get more accuracy out of them.

```{r FINAL1}
# Tuning algorithm #1 - Random Forest
email_notify(paste("Algorithm #1 tuning has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(mtry=c(2,72,142,213,283))
fit.final1 <- train(targetVar~., data=xy_train, method="rf", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.final1)
print(fit.final1)
proc.time()-startTimeModule
email_notify(paste("Algorithm #1 tuning completed!",date()))
```

```{r FINAL2}
# Tuning algorithm #2 - eXtreme Gradient Boosting
email_notify(paste("Algorithm #2 tuning has begun!",date()))
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(nrounds = c(100,250,500,750), max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1, subsample = 1)
fit.final2 <- train(targetVar~., data=xy_train, method="xgbTree", metric=metricTarget, tuneGrid=grid, trControl=control, verbose=F)
plot(fit.final2)
print(fit.final2)
proc.time()-startTimeModule
email_notify(paste("Algorithm #2 tuning completed!",date()))
```

### 5.d) Compare Algorithms After Tuning

```{r POST_TUNING}
results <- resamples(list(RF=fit.final1, XGB=fit.final2))
summary(results)
dotplot(results)
```

## 6. Finalize Model and Present Results
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:

* Using an optimal model tuned to make predictions on unseen data.
* Creating a standalone model using the tuned parameters
* Saving an optimal model to file for later use.

```{r}
email_notify(paste("Model Validation and Final Model Creation has begun!",date()))
```

### 6.a) Predictions on validation dataset

```{r PREDICT}
predictions <- predict(fit.final2, newdata=x_test)
confusionMatrix(predictions, y_test$targetVar)
```

### 6.b) Create standalone model on entire training dataset

```{r}
startTimeModule <- proc.time()
set.seed(seedNum)
#library(xgboost)

# Combining the training and test datasets to form the original dataset that will be used for training the final model
#xy_complete <- rbind(xy_train, xy_test)
#totCol <- ncol(xy_complete)
#x_complete <- xy_complete[,1:totCol]
#y_complete <- xy_complete[,totCol]
#dataMatrix <- data.matrix(x_complete)
#labelMatrix <- data.matrix(y_complete)

#finalModel <- xgboost(data=dataMatrix, label=labelMatrix, nrounds = 600, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1, subsample = 1, verbosity=0)
#summary(finalModel)
proc.time()-startTimeModule
```

### 6.c) Save model for later use

```{r}
#saveRDS(finalModel, "./finalModel_MultiClass.rds")
```

```{r}
email_notify(paste("Model Validation and Final Model Creation Completed!",date()))
proc.time()-startTimeScript
```
