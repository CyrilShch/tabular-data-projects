---
title: "Regression Model for Bike Sharing Using R - Take 2"
author: "David Lowe"
date: "May 29, 2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)

Dataset Used: Bike Sharing Dataset

Dataset ML Model: Regression with numerical attributes

Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset

For performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data

INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration (Take No.2) of the project, we attempt to use the data available, transform as necessary, and apply the Stochastic Gradient Boosting algorithm to examine the modeling effectiveness. Again, the goal of this iteration is to examine various data transformation options and find a sufficiently accurate (low error) combination for future prediction tasks.

This iteration of the project will test the following four modeling scenarios:

Scenario No.1: Remove the attribute "atemp" since it was highly correlated with the attribute "temp."
Scenario No.2: Perform one-hot-encoding on the variable "season."
Scenario No.3: Perform one-hot-encoding on the variable "mnth."

For scenarios 2-3, steps from section No.3 and No.4 will be repeated for each scenario.

CONCLUSION: The baseline performance of the Stochastic Gradient Boosting stands at an RMSE value of 1240 and an R-square value of 0.5943 using the training data. Scenario No.1 did slightly better with an RMSE value of 1233 and an R-square value of 0.5991. As the result, we will leverage scenario No.1 to training the final model and observe how it will do with the validation dataset.

The final Stochastic Gradient Boosting model processed the validation dataset with an RMSE value of 1180 and an R-square value of 0.6320, which was slightly worse than the Take No.1 result of 1177 for RMSE and 0.6329 for R-square. For this iteration of the project, data transformation did not improve the model performance with a noticeable outcome.

The purpose of this project is to analyze predictions using various machine learning algorithms and to document the steps using a template. Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can also encourage us to think about the problem more critically, to challenge our assumptions, and to get proficient at all parts of a modeling project.

The project aims to touch on the following areas:

1. Document a predictive modeling problem end-to-end.
2. Explore data cleaning and transformation options
3. Explore non-ensemble and ensemble algorithms for baseline model performance
4. Explore algorithm tuning techniques for improving model performance

Any predictive modeling machine learning project genrally can be broken down into about six major tasks:

1. Prepare Problem
2. Summarize Data
3. Prepare Data
4. Model and Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(caret)
library(corrplot)
library(dummies)
```

### 1.b) Load dataset

```{r}
startTimeScript <- proc.time()
entireDataset <- read.csv("bike-sharing-day.csv", header= TRUE)

# We are making some preliminary assumptions to drop certain attributes right from the beginning.
# Assumption #1: Drop the "instant" column since it is just an unique row identifier.
# Assumption #2: Drop the "detday" column since it is just an unique date field.
# Assumption #3: Drop the "yr" column since it was just an date representation and a one-time occurance.
# Assumption #4: Drop the "casual" and "registered" columns since they are just derived number from the target variable and directly correlate with it.
entireDataset$instant <- NULL
entireDataset$dteday <- NULL
entireDataset$yr <- NULL
entireDataset$casual <- NULL
entireDataset$registered <- NULL

# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(entireDataset)

# Rename the class/target column to a standard label
colnames(entireDataset)[ncol(entireDataset)] <- "targetVar"

# Save a copy of the entireDataset for re-use by each of the scenarios
entireDataset.save <- entireDataset
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

### 2.a) Descriptive statistics

#### 2.a.i) Peek at the data itself.

```{r}
head(entireDataset)
```

#### 2.a.ii) Dimensions of the dataset.

```{r}
dim(entireDataset)
```

#### 2.a.iii) Types of the attributes.

```{r}
sapply(entireDataset, class)
```

#### 2.a.iv) Statistical summary of all attributes.

```{r}
summary(entireDataset)
```

#### 2.a.v) Count missing values.

```{r}
sapply(entireDataset, function(x) sum(is.na(x)))
```

### 2.b) Data visualizations

#### 2.b.i) Univariate plots to better understand each attribute.

```{r}
# Set up a variable for the total number of attribute columns (totAttr)
totAttr <- totCol-1
# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr
dispCol <- 7
if (totAttr%%dispCol == 0) {
dispRow <- totAttr%/%dispCol
} else {
dispRow <- (totAttr%/%dispCol) + 1
}
```


```{r}
# Boxplots for each attribute
par(mfrow=c(dispRow,dispCol))
for(i in 1:totAttr) {
	boxplot(entireDataset[,i], main=names(entireDataset)[i])
}
```

```{r}
# Histograms each attribute
par(mfrow=c(dispRow,dispCol))
for(i in 1:totAttr) {
	hist(entireDataset[,i], main=names(entireDataset)[i])
}
```

```{r}
# Density plot for each attribute
par(mfrow=c(dispRow,dispCol))
for(i in 1:totAttr) {
	plot(density(entireDataset[,i]), main=names(entireDataset)[i])
}
```

#### 2.b.ii) Multivariate plots to better understand the relationships between attributes

```{r}
# Scatterplot matrix colored by class
pairs(entireDataset[,1:totAttr])
#pairs(targetVar~., data=entireDataset, col=training$targetVar)
```

```{r}
# Correlation plot
correlations <- cor(entireDataset[,1:totAttr])
corrplot(correlations, method="circle")
```

## 3. Prepare Data
Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

### 3.a) Data Cleaning

```{r}
# Not applicable for this iteration of the project.
```

### 3.b) Feature Selection

```{r}
# Not applicable for this iteration of the project.
```

### 3.c) Data Transformation

```{r}
# Not applicable for this iteration of the project.
```

### 3.d) Split-out training and validation datasets
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
seedNum <- 888
set.seed(seedNum)

# Create a list of 70% of the rows in the original dataset we can use for training
training_index <- createDataPartition(entireDataset$targetVar, p=0.70, list=FALSE)

# Use 70% of data to training and testing the models
training <- entireDataset[training_index,]

# Select the remaining 30% of the data for validation
validation <- entireDataset[-training_index,]
```

```{r}
proc.time()-startTimeScript
```

## 4. Model and Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the dataset. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For this iteration of the project, we will evaluate the model using just Stochastic Gradient Boosting.

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Set test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metricTarget <- "RMSE"
```

### 4.b) Generate models
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r GBMBASE}
# Stochastic Gradient Boosting (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.gbm.base <- train(targetVar~., data=training, method="gbm", metric=metricTarget, trControl=control, verbose=F)
print(fit.gbm.base)
proc.time()-startTimeModule
```

## Modeling Scenario #1
For this iteration, we remove the attribute "atemp" since it was highly correlated with the attribute "temp."

### 3.b) Feature Selection

```{r}
entireDataset <- entireDataset.save
entireDataset$atemp <- NULL
```

### 3.d) Split-out training and validation datasets
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
# Use 70% of data to training and testing the models
training1 <- entireDataset[training_index,]

# Select the remaining 30% of the data for validation
validation1 <- entireDataset[-training_index,]
```

### 4.b) Generate models
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r GBM1}
# Stochastic Gradient Boosting (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.gbm.opt1 <- train(targetVar~., data=training1, method="gbm", metric=metricTarget, trControl=control, verbose=F)
print(fit.gbm.opt1)
proc.time()-startTimeModule
```

## End of Modeling Scenario #1

## Modeling Scenario #2
For this iteration, we perform one-hot-encoding on the variable "season."

### 3.c) Data Transforms

```{r}
entireDataset <- entireDataset.save
season_dummy <- dummy(entireDataset$season, sep="_")
entireDataset <- cbind(season_dummy, entireDataset)
entireDataset$season <- NULL
```

### 3.d) Split-out training and validation datasets
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
# Use 70% of data to training and testing the models
training2 <- entireDataset[training_index,]

# Select the remaining 30% of the data for validation
validation2 <- entireDataset[-training_index,]
```

### 4.b) Generate models
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r GBM2}
# Stochastic Gradient Boosting (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.gbm.opt2 <- train(targetVar~., data=training2, method="gbm", metric=metricTarget, trControl=control, verbose=F)
print(fit.gbm.opt2)
proc.time()-startTimeModule
```

## End of Modeling Scenario #2

## Modeling Scenario #3
For this iteration, we perform one-hot-encoding on the variable "mnth."

### 3.c) Data Transforms

```{r}
entireDataset <- entireDataset.save
month_dummy <- dummy(entireDataset$mnth, sep="_")
entireDataset <- cbind(month_dummy, entireDataset)
entireDataset$mnth <- NULL
```

### 3.d) Split-out training and validation datasets
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
# Use 70% of data to training and testing the models
training3 <- entireDataset[training_index,]

# Select the remaining 30% of the data for validation
validation3 <- entireDataset[-training_index,]
```

### 4.b) Generate models
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r GBM3}
# Stochastic Gradient Boosting (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.gbm.opt3 <- train(targetVar~., data=training3, method="gbm", metric=metricTarget, trControl=control, verbose=F)
print(fit.gbm.opt3)
proc.time()-startTimeModule
```

## End of Modeling Scenario #3

## 4. Model and Evaluate Algorithms

### 4.e) Compare baseline and optional models

```{r SPOT_CHECK}
results <- resamples(list(GBMBASE=fit.gbm.base, GBM1=fit.gbm.opt1, GBM2=fit.gbm.opt2, GBM3=fit.gbm.opt3))
summary(results)
dotplot(results)
```

## 5. Improve Accuracy or Results
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models.

Using the three best-perfoming algorithms from the previous section, we will Search for a combination of parameters for each algorithm that yields the best results.

### 5.a) Algorithm Tuning
Finally, we will tune the best-performing algorithms from each group further and see whether we can get more accuracy out of them.

```{r FINAL}
# Reset the dataset to use modeling option #1
entireDataset <- entireDataset.save
entireDataset$atemp <- NULL
# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(entireDataset)
# Use 70% of data to training and testing the models
training <- entireDataset[training_index,]
# Select the remaining 30% of the data for validation
validation <- entireDataset[-training_index,]

# Tuning algorithm - Stochastic Gradient Boosting
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(.n.trees=c(100, 200, 300), .shrinkage=c(0.01, 0.05, 0.1), .interaction.depth=c(1, 2, 3), .n.minobsinnode=c(3, 6, 10))
fit.final <- train(targetVar~., data=training, method="gbm", metric=metricTarget, tuneGrid=grid, trControl=control, verbose=F)
plot(fit.final)
print(fit.final)
proc.time()-startTimeModule
```

## 6. Finalize Model and Present Results
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:

* Using an optimal model tuned to make predictions on unseen data.
* Creating a standalone model using the tuned parameters
* Saving an optimal model to file for later use.

### 6.a) Predictions on validation dataset

```{r PREDICT}
predictions <- predict(fit.final, newdata=validation)
valY <- validation[,totCol]
print(RMSE(predictions, valY))
print(R2(predictions, valY))
```

### 6.b) Create standalone model on entire training dataset

```{r}
library(gbm)
startTimeModule <- proc.time()
set.seed(seedNum)
finalModel <- gbm(targetVar~., entireDataset, distribution = "gaussian", n.trees = 100, interaction.depth = 3, shrinkage = 0.05, n.minobsinnode = 3)
print(finalModel)
proc.time()-startTimeModule
```

### 6.c) Save model for later use

```{r}
#saveRDS(finalModel, "./finalModel_Regression.rds")
```

```{r}
proc.time()-startTimeScript
```
