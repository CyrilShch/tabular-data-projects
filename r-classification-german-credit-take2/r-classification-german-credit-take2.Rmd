---
title: "Binary Classification Model for German Credit Risks Using R Take 2"
author: "David Lowe"
date: "September 14, 2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [http://machinelearningmastery.com/]

SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.

INTRODUCTION: This dataset contains 1,000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.

Because the case study also stipulated that it is worse to classify a customer as good when they are bad (weight of 5), than it is to classify a customer as bad when they are good (weight of 1). For this iteration, the script focuses on tuning various machine learning algorithms and identify the algorithm that can produce the best cost-and-accuracy tradeoffs.

CONCLUSION: From the previous iteration Take 1, The baseline performance of the eight algorithms achieved an average accuracy of 72.69%. Three algorithms (Logistic Regression, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 75.00%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 75.67%, which was slightly better than the accuracy from the training data.

From the cost vs accuracy comparison, both the Logistic Regression and AdaBoost achieved identical accuracy while keeping the costs of incorrect predictions low. Either algorithm should be considered for further modeling or production use.

Dataset Used: German Credit Data Set

Dataset ML Model: Binary classification with numerical and categorical attributes

Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29

One potential source of performance benchmarks: https://www.kaggle.com/uciml/german-credit/home

The project aims to touch on the following areas:

1. Document a predictive modeling problem end-to-end.
2. Explore data cleaning and transformation options
3. Explore non-ensemble and ensemble algorithms for baseline model performance
4. Explore algorithm tuning techniques for improving model performance

Any predictive modeling machine learning project genrally can be broken down into about six major tasks:

1. Prepare Problem
2. Summarize Data
3. Prepare Data
4. Model and Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(caret)
library(corrplot)
library(ROCR)
```

### 1.b) Load dataset

```{r}
startTimeScript <- proc.time()
entireDataset <- read.table("german.data",
                            col.names=c("attr01","attr02","attr03","attr04","attr05","attr06","attr07","attr08","attr09","attr10","attr11","attr12","attr13","attr14","attr15","attr16","attr17","attr18","attr19","attr20","targetVar"))
# Re-label the class to "0" - Positive Class/Good Loan and "1" - Negative Class/Bad Loan
entireDataset$targetVar <- entireDataset$targetVar-1
entireDataset$targetVar <- as.factor(entireDataset$targetVar)
```

### 1.c) Set up key parameters to be used in the script

```{r}
# Create one random seed number for reproducible results
seedNum <- 888
set.seed(seedNum)

# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(entireDataset)
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

### 2.a) Descriptive statistics

#### 2.a.i) Peek at the data itself.

```{r}
head(entireDataset)
```

#### 2.a.ii) Dimensions of the dataset.

```{r}
dim(entireDataset)
```

#### 2.a.iii) Types of the attributes.

```{r}
sapply(entireDataset, class)
```

#### 2.a.iv) Statistical summary of all attributes.

```{r}
summary(entireDataset)
```

#### 2.a.v) Summarize the levels of the class attribute.

```{r}
entireDataset_x <- entireDataset[,1:(totCol-1)]
entireDataset_y <- entireDataset[,totCol]
cbind(freq=table(entireDataset_y), percentage=prop.table(table(entireDataset_y))*100)
```

#### 2.a.vi) Count missing values.

```{r}
sapply(entireDataset, function(x) sum(is.na(x)))
```

## 3. Prepare Data
Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

### 3.a) Data Cleaning

```{r}
# Not applicable for this iteration of the project.

# Mark missing values
#invalid <- 0
#entireDataset$some_col[entireDataset$some_col==invalid] <- NA

# Impute missing values
#entireDataset$some_col <- with(entireDataset, impute(some_col, mean))
```

### 3.b) Feature Selection

```{r}
# Not applicable for this iteration of the project.
```

### 3.c) Data Transforms

```{r}
# Not applicable for this iteration of the project.
```

### 3.d) Split-out training and validation datasets
We create a training dataset (variable name "training") and a validation dataset (variable name "validation").

```{r}
# Create a list of the rows in the original dataset we can use for training
training_index <- createDataPartition(entireDataset$targetVar, p=0.70, list=FALSE)

# Use 70% of data to training and testing the models
training <- entireDataset[training_index,]

# Select the remaining 30% of the data for validation
validation <- entireDataset[-training_index,]
```

```{r}
proc.time()-startTimeScript
```

## 4. Model and Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the training. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For this project, we will evaluate one linear, three non-linear, and four ensemble algorithms:

Linear Algorithm: Logistic Regression

Non-Linear Algorithms: Decision Trees (CART), k-Nearest Neighbors, and Support Vector Machine

Ensemble Algorithms: Bagged CART, Random Forest, Adaboost, and Stochastic Gradient Boosting

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Set test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metricTarget <- "Accuracy"
```

### 4.b) Generate model and calculate costs - Logistic Regression

```{r LR_Model}
# Logistic Regression (Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.glm <- train(targetVar~., data=training, method="glm", metric=metricTarget, trControl=control)
print(fit.glm)
```

```{r LR_Predict}
predictions <- predict(fit.glm, newdata=validation)
cm_LR <- confusionMatrix(predictions, validation$targetVar)
cm_LR
proc.time()-startTimeModule
```

### 4.c) Generate model and calculate costs - Decision Tree

```{r CART_Model}
# Decision Tree - CART (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(cp = c(0.005, 0.01, 0.015, 0.02, 0.025))
fit.cart <- train(targetVar~., data=training, method="rpart", metric=metricTarget, tuneGrid=grid, trControl=control)
#fit.cart <- train(targetVar~., data=training, method="rpart", metric=metricTarget, trControl=control)
plot(fit.cart)
print(fit.cart)
```

```{r CART_Predict}
predictions <- predict(fit.cart, newdata=validation)
cm_CART <- confusionMatrix(predictions, validation$targetVar)
cm_CART
proc.time()-startTimeModule
```

### 4.d) Generate model and calculate costs - k-Nearest Neighbors

```{r KNN_Model}
# k-Nearest Neighbors (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(k = c(30, 40, 50, 60, 70))
fit.knn <- train(targetVar~., data=training, method="knn", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.knn)
print(fit.knn)
```

```{r KNN_Predict}
predictions <- predict(fit.knn, newdata=validation)
cm_KNN <- confusionMatrix(predictions, validation$targetVar)
cm_KNN
proc.time()-startTimeModule
```

### 4.e) Generate model and calculate costs - Support Vector Machine

```{r SVM_Model}
# Support Vector Machine (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(sigma = c(0.01, 0.02, 0.03), C = c(0.5, 0.75, 1.0, 1.25, 1.5))
fit.svm <- train(targetVar~., data=training, method="svmRadial", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.svm)
print(fit.svm)
```

```{r SVM_Predict}
predictions <- predict(fit.svm, newdata=validation)
cm_SVM <- confusionMatrix(predictions, validation$targetVar)
cm_SVM
proc.time()-startTimeModule
```

### 4.f) Generate model and calculate costs - Bagged CART

```{r BAGCART_Model}
# Bagged CART (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.bagcart <- train(targetVar~., data=training, method="treebag", metric=metricTarget, trControl=control)
print(fit.bagcart)
```

```{r BAGCART_Predict}
predictions <- predict(fit.bagcart, newdata=validation)
cm_BAGCART <- confusionMatrix(predictions, validation$targetVar)
cm_BAGCART
proc.time()-startTimeModule
```

### 4.g) Generate model and calculate costs - Random Forest

```{r RF_Model}
# Random Forest (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(mtry = c(2, 4, 6, 8, 10))
fit.rf <- train(targetVar~., data=training, method="rf", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.rf)
print(fit.rf)
```

```{r RF_Predict}
predictions <- predict(fit.rf, newdata=validation)
cm_RF <- confusionMatrix(predictions, validation$targetVar)
cm_RF
proc.time()-startTimeModule
```

### 4.h) Generate model and calculate costs - AdaBoost

```{r ADA_Model}
# AdaBoost (Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(iter=c(200, 300, 400), nu=c(0.01, 0.05, 0.1), maxdepth=c(2, 3, 4))
fit.ada <- train(targetVar~., data=training, method="ada", metric=metricTarget, tuneGrid=grid, trControl=control)
plot(fit.ada)
print(fit.ada)
```

```{r ADA_Predict}
predictions <- predict(fit.ada, newdata=validation)
cm_ADA <- confusionMatrix(predictions, validation$targetVar)
cm_ADA
proc.time()-startTimeModule
```

### 4.i) Generate model and calculate costs - Stochastic Gradient Boosting

```{r GBM}
# Stochastic Gradient Boosting (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(.n.trees=c(100, 200, 300), .shrinkage=c(0.01, 0.05, 0.1), .interaction.depth=c(1, 2, 3), .n.minobsinnode=c(3, 5, 7))
fit.gbm <- train(targetVar~., data=training, method="gbm", metric=metricTarget, tuneGrid=grid, trControl=control, verbose=F)
plot(fit.gbm)
print(fit.gbm)
```

```{r GBM_Predict}
predictions <- predict(fit.gbm, newdata=validation)
cm_GBM <- confusionMatrix(predictions, validation$targetVar)
cm_GBM
proc.time()-startTimeModule
```

## 5. Model Comparisons

### 5.a) Compare the accuracy of models

```{r SPOT_CHECK}
results <- resamples(list(LR=fit.glm, CART=fit.cart, kNN=fit.knn, SVM=fit.svm, BagCART=fit.bagcart, RF=fit.rf, AdaBoost=fit.ada, GBM=fit.gbm))
summary(results)
dotplot(results)
```

### 5.a) Compare the cost of models

```{r COMPARE_COST}
# Calculate the costs of wrong predictions
# Weight of 1 for false negative and weight of 5 for false positive
weight_fn <- 1
weight_fp <- 5
cost_LR <- (cm_LR$table[1,2]*weight_fp) + (cm_LR$table[2,1]*weight_fn)
cost_CART <- (cm_CART$table[1,2]*weight_fp) + (cm_CART$table[2,1]*weight_fn)
cost_KNN <- (cm_KNN$table[1,2]*weight_fp) + (cm_KNN$table[2,1]*weight_fn)
cost_SVM <- (cm_SVM$table[1,2]*weight_fp) + (cm_SVM$table[2,1]*weight_fn)
cost_BAGCART <- (cm_BAGCART$table[1,2]*weight_fp) + (cm_BAGCART$table[2,1]*weight_fn)
cost_RF <- (cm_RF$table[1,2]*weight_fp) + (cm_RF$table[2,1]*weight_fn)
cost_ADA <- (cm_ADA$table[1,2]*weight_fp) + (cm_ADA$table[2,1]*weight_fn)
cost_GBM <- (cm_GBM$table[1,2]*weight_fp) + (cm_GBM$table[2,1]*weight_fn)
costs <- c(cost_LR, cost_CART, cost_KNN, cost_SVM, cost_BAGCART, cost_RF, cost_ADA, cost_GBM)
models <- c("LR", "CART", "KNN", "SVM", "BGCRT", "RF", "ADA", "GBM")
costs_compare <- data.frame(costs, models)
plot_original <- barplot(costs_compare[,1], names.arg=costs_compare[,2], ylab = "Costs of FP & FN", ylim = range(0,500))
text(x=plot_original, y=costs_compare[,1], label=costs_compare[,1], pos=3, col="red")
```

```{r SORT_COST}
costs_sorted <- costs_compare[order(costs_compare[,1], decreasing = FALSE),]
plot_sorted <- barplot(costs_sorted[,1], names.arg=costs_sorted[,2], ylab = "Costs of FP & FN", ylim = range(0,500))
text(x=plot_sorted, y=costs_sorted[,1], label=costs_sorted[,1], pos=3, col="red")
```

### 5.c) Plot the cost vs. accuracy of models

```{r COST_VS_ACCURACY}
accuracy_LR <- cm_LR$overall[['Accuracy']]
accuracy_CART <- cm_CART$overall[['Accuracy']]
accuracy_KNN <- cm_KNN$overall[['Accuracy']]
accuracy_SVM <- cm_SVM$overall[['Accuracy']]
accuracy_BAGCART <- cm_BAGCART$overall[['Accuracy']]
accuracy_RF <- cm_RF$overall[['Accuracy']]
accuracy_ADA <- cm_ADA$overall[['Accuracy']]
accuracy_GBM <- cm_GBM$overall[['Accuracy']]
ratios <- c(accuracy_LR,accuracy_CART,accuracy_KNN,accuracy_SVM,accuracy_BAGCART,accuracy_RF,accuracy_ADA,accuracy_GBM)
costs_ratios <- data.frame(costs, ratios, models)
plot_vs <- plot(costs_ratios[,1], costs_ratios[,2], main="Accuracy vs. Costs Scatterplot", xlab="Weighted Costs", ylab="Accuracy %", ylim=range(0.675,0.8))
text(x=costs_ratios[,1], y=costs_ratios[,2], labels=costs_ratios[,3], pos=3)
```

```{r}
proc.time()-startTimeScript
```
