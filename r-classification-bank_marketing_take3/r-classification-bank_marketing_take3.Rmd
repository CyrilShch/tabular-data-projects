---
title: "Binary Classification Model for Bank Marketing Using R Take #3"
author: "David Lowe"
date: "May 11, 2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)

Dataset Used: Bank Marketing Data Set

Data Set ML Model: Binary classification with numerical and categorical attributes

Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing

One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced

INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 41,000 observations with 19 input variables and 1 output variable. There are no missing values within the dataset. This dataset is based on "Bank Marketing" UCI dataset and is enriched by the addition of five new social and economic features/attributes. This dataset is almost identical to the one without the five new attributes.

CONCLUSION: The take No.3 version of this banking dataset aims to test the addition of five additional social-economical attributes to the dataset and the effect. You can see the results from the take No.2 here on the website: https://daines-analytics.com/wp-content/uploads/2018/05/r-classification-Bank_Marketing_Take2.html.

The baseline performance of the seven algorithms achieved an average accuracy of 89.70% (vs. 89.22% from the take No.2 version). Three algorithms (Logistic Regression, Bagged CART, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieve the top accuracy/Kappa result using the training data. It produced an average accuracy of 90.11% (vs. 89.46% from the take No.2 version) using the training data.

Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 90.00%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The addition of the social-economical attributes did not seem to have a substantial effect on the overall accuracy of the prediction models.

The purpose of this project is to analyze predictions using various machine learning algorithms and to document the steps using a template. Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can also encourage us to think about the problem more critically, to challenge our assumptions, and to get proficient at all parts of a modeling project.

The project aims to touch on the following areas:

1. Document a predictive modeling problem end-to-end.
2. Explore data cleaning and transformation options
3. Explore non-ensemble and ensemble algorithms for baseline model performance
4. Explore algorithm tuning techniques for improving model performance

Any predictive modeling machine learning project genrally can be broken down into about six major tasks:

1. Prepare Problem
2. Summarize Data
3. Prepare Data
4. Model and Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(caret)
library(ROCR)
```

### 1.b) Load training

```{r}
startTimeScript <- proc.time()
entireDataset <- read.csv2("bank-additional-full.csv", header= TRUE)

#Dropping the duration column to prevent future from leaking into the present
entireDataset$duration <- NULL

#Adjusting some factor columns back into numeric
entireDataset$emp.var.rate <- as.numeric(as.character(entireDataset$emp.var.rate))
entireDataset$cons.price.idx <- as.numeric(as.character(entireDataset$cons.price.idx))
entireDataset$cons.conf.idx <- as.numeric(as.character(entireDataset$cons.conf.idx))
entireDataset$euribor3m <- as.numeric(as.character(entireDataset$euribor3m))
entireDataset$nr.employed <- as.numeric(as.character(entireDataset$nr.employed))

# Use variable totCol to hold the number of columns in the dataframe
totCol <- ncol(entireDataset)

# Rename the class column to a standard label
colnames(entireDataset)[ncol(entireDataset)] <- "targetVar"
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

### 2.a) Descriptive statistics

#### 2.a.i) Peek at the data itself.

```{r}
head(entireDataset)
```

#### 2.a.ii) Dimensions of the dataset.

```{r}
dim(entireDataset)
```

#### 2.a.iii) Types of the attributes.

```{r}
sapply(entireDataset, class)
```

#### 2.a.iv) Statistical summary of all attributes.

```{r}
summary(entireDataset)
```

#### 2.a.v) Summarize the levels of the class attribute.

```{r}
entireDataset_x <- entireDataset[,1:(totCol-1)]
entireDataset_y <- entireDataset[,totCol]
cbind(freq=table(entireDataset_y), percentage=prop.table(table(entireDataset_y))*100)
```

#### 2.a.vi) Count missing values.

```{r}
sapply(entireDataset, function(x) sum(is.na(x)))
```

### 2.b) Data visualizations

#### 2.b.i) Univariate plots to better understand each attribute.

```{r}
# Set up a variable for the total number of attribute columns (totAttr)
totAttr <- totCol-1
# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr
dispCol <- 4
if (totAttr%%dispCol == 0) {
dispRow <- totAttr%/%dispCol
} else {
dispRow <- (totAttr%/%dispCol) + 1
}
```


```{r}
# Boxplots for each attribute
#par(mfrow=c(dispRow,dispCol))
#for(i in 1:totAttr) {
#	boxplot(entireDataset[,i], main=names(entireDataset)[i])
#}
```

```{r}
# Histograms each attribute
#par(mfrow=c(dispRow,dispCol))
#for(i in 1:totAttr) {
#	hist(entireDataset[,i], main=names(entireDataset)[i])
#}
```

```{r}
# Density plot for each attribute
#par(mfrow=c(dispRow,dispCol))
#for(i in 1:totAttr) {
#	plot(density(entireDataset[,i]), main=names(entireDataset)[i])
#}
```

#### 2.b.ii) Multivariate plots to better understand the relationships between attributes

```{r}
# Scatterplot matrix colored by class
#pairs(targetVar~., data=entireDataset, col=training$targetVar)
```

```{r}
# Box and whisker plots for each attribute by class
#scales <- list(x=list(relation="free"), y=list(relation="free"))
#featurePlot(x=entireDataset_x, y=entireDataset_y, plot="box", scales=scales)
```

```{r}
# Density plots for each attribute by class value
#featurePlot(x=entireDataset_x, y=entireDataset_y, plot="density", scales=scales)
```

```{r}
# Correlation plot
#correlations <- cor(entireDataset_x)
#corrplot(correlations, method="circle")
```

## 3. Prepare Data
Some training may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

### 3.a) Data Cleaning

```{r}
# Not applicable for this iteration of the project.

# Mark missing values
#invalid <- 0
#entireDataset$some_col[entireDataset$some_col==invalid] <- NA

# Impute missing values
#entireDataset$some_col <- with(entireDataset, impute(some_col, mean))
```

### 3.b) Feature Selection

```{r}
# Not applicable for this iteration of the project.
```

### 3.c) Data Transforms

```{r}
# Not applicable for this iteration of the project.
```

### 3.d) Split-out training and validation trainings
We create a training training (variable name "training") and a validation training (variable name "validation").

```{r}
seedNum <- 888
set.seed(seedNum)

# Create a list of 70% of the rows in the original training we can use for training
training_index <- createDataPartition(entireDataset$targetVar, p=0.70, list=FALSE)

# Use 70% of data to training and testing the models
training <- entireDataset[training_index,]

# Select the remaining 30% of the data for validation
validation <- entireDataset[-training_index,]
```

## 4. Model and Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the training. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For this project, we will evaluate two linear, three non-linear, and three ensemble algorithms:

Linear Algorithm: Logistic Regression

Non-Linear Algorithms: Decision Trees (CART), k-Nearest Neighbors, and Support Vector Machine

Ensemble Algorithms: Bagged CART, Random Forest, and Stochastic Gradient Boosting

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Set test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metricTarget <- "Accuracy"
```

### 4.b) Generate models using linear algorithms

```{r LR}
# Logistic Regression (Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.glm <- train(targetVar~., data=training, method="glm", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

### 4.c) Generate models using nonlinear algorithms

```{r CART}
# Decision Tree - CART (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.cart <- train(targetVar~., data=training, method="rpart", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r KNN}
# k-Nearest Neighbors (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.knn <- train(targetVar~., data=training, method="knn", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r SVM}
# Support Vector Machine (Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.svm <- train(targetVar~., data=training, method="svmRadial", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

### 4.d) Generate models using ensemble algorithms
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r BAGCART}
# Bagged CART (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.bagcart <- train(targetVar~., data=training, method="treebag", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r RF}
# Random Forest (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.rf <- train(targetVar~., data=training, method="rf", metric=metricTarget, trControl=control)
proc.time()-startTimeModule
```

```{r GBM}
# Stochastic Gradient Boosting (Regression/Classification)
startTimeModule <- proc.time()
set.seed(seedNum)
fit.gbm <- train(targetVar~., data=training, method="gbm", metric=metricTarget, trControl=control, verbose=F)
proc.time()-startTimeModule
```

### 4.e) Compare baseline algorithms

```{r SPOT_CHECK}
results <- resamples(list(LogReg=fit.glm, CART=fit.cart, kNN=fit.knn, SVM=fit.svm, BagCART=fit.bagcart, RF=fit.rf, GBM=fit.gbm))
summary(results)
dotplot(results)
```

## 5. Improve Accuracy or Results
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models:

* Search for a combination of parameters for each algorithm using caret that yields the best results.
* Combine the prediction of multiple models into an ensemble prediction using standalone algorithms or the caret Ensemble package.

### 5.a) Algorithm Tuning
Finally, we will tune the top three trained algorithms and see whether we can achieve an even better result with them.

```{r FINAL1}
# Tuning algorithm #1 - Logistic Regression
# Logistic Regression requires no special tuning parameters with Caret
startTimeModule <- proc.time()
set.seed(seedNum)
fit.final1 <- fit.glm
print(fit.final1)
proc.time()-startTimeModule
```

```{r FINAL2}
# Tuning algorithm #2 - Bagged CART
# Bagged CART requires no special tuning parameters with Caret
startTimeModule <- proc.time()
set.seed(seedNum)
fit.final2 <- fit.bagcart
print(fit.final2)
proc.time()-startTimeModule
```

```{r FINAL3}
# Tuning algorithm #3 - Stochastic Gradient Boosting
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(.n.trees=c(100, 250, 500), .interaction.depth=c(1, 2, 3), .shrinkage=c(0.001, 0.025, 0.05), .n.minobsinnode=c(10, 20, 30))
fit.final3 <- train(targetVar~., data=training, method="gbm", metric=metricTarget, tuneGrid=grid, trControl=control, verbose=F)
plot(fit.final3)
print(fit.final3)
proc.time()-startTimeModule
```

### 5.d) Compare Algorithms After Tuning

```{r POST_TUNING}
results <- resamples(list(LR=fit.final1, BagCART=fit.final2, GBM=fit.final3))
summary(results)
dotplot(results)
```

## 6. Finalize Model and Present Results
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:

* Using an optimal model tuned to make predictions on unseen data.
* Creating a standalone model using the tuned parameters
* Saving an optimal model to file for later use.

### 6.a) Predictions on validation training

```{r PREDICT}
predictClass <- predict(fit.final3, newdata=validation)
confusionMatrix(predictClass, validation$targetVar)

pred <- prediction(as.numeric(predictClass), as.numeric(validation$targetVar))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, colorize=TRUE)

auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

### 6.b) Create standalone model from the entire training dataset

```{r}
library(gbm)
startTimeModule <- proc.time()
set.seed(seedNum)
grid <- expand.grid(.n.trees=500, .interaction.depth=3, .shrinkage=0.05, .n.minobsinnode=20)
finalModel <- train(targetVar~., data=entireDataset, method="gbm", metric=metricTarget, tuneGrid=grid, trControl=control, verbose=F)
print(finalModel)
proc.time()-startTimeModule
```

### 6.c) Save model for later use

```{r}
#saveRDS(finalModel, "./finalModel_BinaryClass.rds")
```

```{r}
proc.time()-startTimeScript
```