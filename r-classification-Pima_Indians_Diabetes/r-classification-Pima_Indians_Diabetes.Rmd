---
title: "Simple Classification Model for Diabetes Prediction Using R"
author: "David Lowe"
date: "April 6, 2018"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)

Dataset Used: Pima Indians Diabetes Database 
Data Set ML Model: Classification with numerical attributes
Dataset Reference: https://www.kaggle.com/uciml/pima-indians-diabetes-database

For more information on this case study project, please consult Dr. Brownlee's blog post at https://machinelearningmastery.com/standard-machine-learning-datasets/.

For more information on performance benchmarks, please consult: https://www.kaggle.com/uciml/pima-indians-diabetes-database

INTRODUCTION: The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details. It is a binary (2-class) classification problem. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values.

CONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 75.85%. The top accuracy result achieved via Logistic Regression was 77.73% after a series of tuning trials. The ensemble algorithms, in this case, did not yield a better result than the non-ensemble algorithms to justify the additional processing required.

The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:

1. Document a predictive modeling problem end-to-end.
2. Explore data transformation options for improving model performance
3. Explore non-ensemble and ensemble algorithms for improving model performance
4. Explore algorithm tuning techniques for improving model performance

Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.

Any predictive modeling machine learning project can be broken down into about 6 common tasks:

1. Define Problem
2. Summarize Data
3. Prepare Data
4. Evaluate Algorithms
5. Improve Accuracy or Results
6. Finalize Model and Present Results

## 1. Prepare Problem

### 1.a) Load libraries

```{r}
library(mlbench)
library(caret)
library(corrplot)
```

### 1.b) Load dataset

```{r}
seedNum <- 888
data(PimaIndiansDiabetes)
dataset <- PimaIndiansDiabetes
# Rename the class column to a standard label
colnames(dataset)[9] <- "classVar"
```

### 1.c) Split-out validation dataset
Normally, we would create a training (variable name "dataset") and a validation (variable name "validation") dataset. Because this dataset is relatively small, we will opt to test the algorithms with the full set of data and not splitting.

```{r}
# Not applicable for this iteration of the project.
```

## 2. Summarize Data
To gain a better understanding of the data that we have on-hand, we will leverage a number of descriptive statistics and data visualization techniques. The plan is to use the results to consider new questions, review assumptions, and validate hypotheses that we can investigate later with specialized models.

### 2.a) Descriptive statistics

#### 2.a.i) Dimensions of the dataset.

```{r}
dim(dataset)
```

#### 2.a.ii) Types of the attributes.

```{r}
sapply(dataset, class)
```

#### 2.a.iii) Peek at the data itself.

```{r}
head(dataset)
```

#### 2.a.iv) Summarize the levels of the class attribute.

```{r}
x <- dataset[,1:8]
y <- dataset[,9]
cbind(freq=table(y), percentage=prop.table(table(y))*100)
```

#### 2.a.v) Summarize correlations between input variables.

```{r}
cor(x)
```

#### 2.a.vi) Statistical summary of all attributes.

```{r}
summary(dataset)
```

### 2.b) Data visualizations

#### 2.b.i) Univariate plots to better understand each attribute.

```{r}
# boxplots for each attribute
par(mfrow=c(2,4))
for(i in 1:8) {
	boxplot(dataset[,i], main=names(dataset)[i])
}
```

```{r}
# histograms each attribute
par(mfrow=c(2,4))
for(i in 1:8) {
	hist(dataset[,i], main=names(dataset)[i])
}
```

```{r}
# density plot for each attribute
par(mfrow=c(2,4))
for(i in 1:8) {
	plot(density(dataset[,i]), main=names(dataset)[i])
}
```

#### 2.b.ii) Multivariate plots to better understand the relationships between attributes

```{r}
# scatterplot matrix colored by class
pairs(classVar~., data=dataset, col=dataset$classVar)
```

```{r}
# box and whisker plots for each attribute by class
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="box", scales=scales)
```

```{r}
# density plots for each attribute by class value
featurePlot(x=x, y=y, plot="density", scales=scales)
```

```{r}
# correlation plot
correlations <- cor(x)
corrplot(correlations, method="circle")
```

## 3. Prepare Data
Some dataset may require additional preparation activities that will best exposes the structure of the problem and the relationships between the input attributes and the output variable. Some data-prep tasks might include:

* Cleaning data by removing duplicates, marking missing values and even imputing missing values.
* Feature selection where redundant features may be removed.
* Data transforms where attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms.

### 3.a) Data Cleaning

```{r}
# Not applicable for this iteration of the project.
# Mark missing values
# invalid <- 0
# dataset$pressure[dataset$pressure==invalid] <- NA
# dataset$mass[dataset$mass==invalid] <- NA
# dataset$glucose[dataset$glucose==invalid] <- NA
# # Impute missing values
# dataset$pressure <- with(dataset, impute(pressure, mean))
# dataset$mass <- with(dataset, impute(mass, mean))
# dataset$glucose <- with(dataset, impute(glucose, mean))
```

### 3.b) Feature Selection

```{r}
# Not applicable for this iteration of the project.
```

### 3.c) Data Transforms

```{r}
# Not applicable for this iteration of the project.
```

## 4. Evaluate Algorithms
After the data-prep, we next work on finding a workable model by evaluating a subset of machine learning algorithms that are good at exploiting the structure of the dataset. The typical evaluation tasks include:

* Defining test options such as cross validation and the evaluation metric to use.
* Spot checking a suite of linear and nonlinear machine learning algorithms.
* Comparing the estimated accuracy of algorithms.

For this project, we will evaluate four non-ensemble and three ensemble algorithms:

Non-Ensemble Algorithms: Logistic Regression, Linear Discriminant Analysis, Decision Trees (CART), Naive Bayes, k-Nearest Neighbors, and Support Vector Machine

Ensemble Algorithms: Bagged CART, Random Forest, Adaboost, Stochastic Gradient Boosting, and Neural Networks

The random number seed is reset before each run to ensure that the evaluation of each algorithm is performed using the same data splits. It ensures the results are directly comparable.

### 4.a) Test options and evaluation metric

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metricTarget <- "Accuracy"
```

### 4.b) Generate Models using Non-Ensemble Algorithms

```{r}
# Linear Regression (Regression)
# Algorithm not applicable for this project
#set.seed(seedNum)
#fit.lm <- train(classVar~., data=dataset, method="lm", metric=metricTarget, trControl=control)
```

```{r}
# Logistic Regression (Regression/Classification)
set.seed(seedNum)
fit.glm <- train(classVar~., data=dataset, method="glm", metric=metricTarget, trControl=control)
```

```{r}
# Linear/Quadratic Discriminant Analysis (Regression/Classification)
set.seed(seedNum)
fit.lda <- train(classVar~., data=dataset, method="lda", metric=metricTarget, trControl=control)
```

```{r}
# Decision Tree - CART (Regression/Classification)
set.seed(seedNum)
fit.cart <- train(classVar~., data=dataset, method="rpart", metric=metricTarget, trControl=control)
```

```{r}
# Naive Bayes (Classification)
set.seed(seedNum)
fit.nb <- train(classVar~., data=dataset, method="nb", metric=metricTarget, trControl=control)
```

```{r}
# k-Nearest Neighbors (Regression/Classification)
set.seed(seedNum)
fit.knn <- train(classVar~., data=dataset, method="knn", metric=metricTarget, trControl=control)
```

```{r}
# Support Vector Machine (Classification)
set.seed(seedNum)
fit.svm <- train(classVar~., data=dataset, method="svmRadial", metric=metricTarget, trControl=control)
```

### 4.C) Generate Models using Ensemble Algorithms
In this section, we will explore the use and tuning of ensemble algorithms to see whether we can improve the results.

```{r}
# Bagged CART (Regression/Classification)
set.seed(seedNum)
fit.bagcart <- train(classVar~., data=dataset, method="treebag", metric=metricTarget, trControl=control)
```

```{r}
# Random Forest (Regression/Classification)
set.seed(seedNum)
fit.rf <- train(classVar~., data=dataset, method="rf", metric=metricTarget, trControl=control)
```

```{r}
# AdaBoost (Classification)
set.seed(seedNum)
fit.ada <- train(classVar~., data=dataset, method="adaboost", metric=metricTarget, trControl=control)
```

```{r}
# Stochastic Gradient Boosting (Regression/Classification)
set.seed(seedNum)
fit.gbm <- train(classVar~., data=dataset, method="gbm", metric=metricTarget, trControl=control)
```

```{r}
# Neural Network (Regression/Classification)
set.seed(seedNum)
fit.nnet <- train(classVar~., data=dataset, method="nnet", metric=metricTarget, trControl=control)
```

### 4.d) Compare Algorithms Before Tuning

```{r}
#results <- resamples(list(LM=fit.lm, LogReg=fit.glm, LDA=fit.lda, CART=fit.cart, NB=fit.nb, kNN=fit.knn, SVM=fit.svm, BagCART=fit.bagcart, RF=fit.rf, AdaBoost=fit.ada, GBM=fit.gbm, NNet=fit.nnet))
results <- resamples(list(LogReg=fit.glm, LDA=fit.lda, CART=fit.cart, NB=fit.nb, kNN=fit.knn, SVM=fit.svm, BagCART=fit.bagcart, RF=fit.rf, AdaBoost=fit.ada, GBM=fit.gbm, NNet=fit.nnet))
summary(results)
dotplot(results)
```

## 5. Improve Accuracy or Results
After we achieve a short list of machine learning algorithms with good level of accuracy, we can leverage ways to improve the accuracy of the models:

* Search for a combination of parameters for each algorithm using caret that yields the best results.
* Combine the prediction of multiple models into an ensemble prediction using standalone algorithms or the caret Ensemble package.

### 5.a) Algorithm Tuning
Finally, we will tune the three best-performing algorithms further and see whether we can get more accuracy out of them.

```{r}
# Tuning algorithm #1 - Logistic Regression
# GLM requires no tuning parameters.
set.seed(seedNum)
fit.final1 <- fit.glm
print(fit.final1)
```

```{r}
# Tuning algorithm #2 - Linear/Quadratic Discriminant Analysis
# LDA requires no tuning parameters.
set.seed(seedNum)
fit.final2 <- fit.lda
print(fit.final2)
```

```{r}
# Tuning algorithm #3 - Stochastic Gradient Boosting
set.seed(seedNum)
fit.final3 <- fit.lda
grid <- expand.grid(.n.trees=c(100, 250, 500), .shrinkage=c(0, 0.001, 0.1, 0.2, 0.3, 0.5, 1), .interaction.depth=c(1,2,3), .n.minobsinnode=c(1, 2, 3))
fit.final3 <- train(classVar~., data=dataset, method="gbm", metric="Accuracy", tuneGrid=grid, trControl=control)
plot(fit.final3)
print(fit.final3)
```

### 5.d) Compare Algorithms After Tuning

```{r}
results <- resamples(list(LogReg=fit.final1, LDA=fit.final2, GBM=fit.final3))
summary(results)
dotplot(results)
```

## 6. Finalize Model and Present Results
Once we have narrow down to a model that we believe can make accurate predictions on unseen data, we are ready to finalize it. Finalizing a model may involve sub-tasks such as:

* Using an optimal model tuned to make predictions on unseen data.
* Creating a standalone model using the tuned parameters
* Saving an optimal model to file for later use.

For this particular dataset, we will settle on using the SVM algorithm with a sigma value of 0.01.

### 6.a) Predictions on validation dataset

```{r}
# Not applicable for this iteration of the project.
```

### 6.b) Create standalone model on entire training dataset

```{r}
# Not applicable for this iteration of the project.
```

### 6.c) Save model for later use

```{r}
# Not applicable for this iteration of the project.
```
